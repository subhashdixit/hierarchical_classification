{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## **<center>Hierarchical Classification</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Subhash\n",
      "[nltk_data]     Dixit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "# data = data.iloc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Basic analysis and pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>Title</th>\n",
       "      <th>userId</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cat1</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>Cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0002AQK70</td>\n",
       "      <td>PetSafe Staywell Pet Door with Clear Hard Flap</td>\n",
       "      <td>A2L6QTQQI13LZG</td>\n",
       "      <td>1344211200</td>\n",
       "      <td>We've only had it installed about 2 weeks. So ...</td>\n",
       "      <td>pet supplies</td>\n",
       "      <td>cats</td>\n",
       "      <td>cat flaps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0002DK8OI</td>\n",
       "      <td>Kaytee Timothy Cubes, 1-Pound</td>\n",
       "      <td>A2HJUOZ9R9K4F</td>\n",
       "      <td>1344211200</td>\n",
       "      <td>My bunny had a hard time eating this because t...</td>\n",
       "      <td>pet supplies</td>\n",
       "      <td>bunny rabbit central</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0006VJ6TO</td>\n",
       "      <td>Body Back Buddy</td>\n",
       "      <td>A14PK96LL78NN3</td>\n",
       "      <td>1344211200</td>\n",
       "      <td>would never in a million years have guessed th...</td>\n",
       "      <td>health personal care</td>\n",
       "      <td>health care</td>\n",
       "      <td>massage relaxation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000EZSFXA</td>\n",
       "      <td>SnackMasters California Style Turkey Jerky</td>\n",
       "      <td>A2UW73HU9UMOTY</td>\n",
       "      <td>1344211200</td>\n",
       "      <td>Being the jerky fanatic I am, snackmasters han...</td>\n",
       "      <td>grocery gourmet food</td>\n",
       "      <td>snack food</td>\n",
       "      <td>jerky dried meats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000KV61FC</td>\n",
       "      <td>Premier Busy Buddy Tug-a-Jug Treat Dispensing ...</td>\n",
       "      <td>A1Q99RNV0TKW8R</td>\n",
       "      <td>1344211200</td>\n",
       "      <td>Wondered how quick my dog would catch on to th...</td>\n",
       "      <td>pet supplies</td>\n",
       "      <td>dogs</td>\n",
       "      <td>toys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productId                                              Title  \\\n",
       "0  B0002AQK70     PetSafe Staywell Pet Door with Clear Hard Flap   \n",
       "1  B0002DK8OI                      Kaytee Timothy Cubes, 1-Pound   \n",
       "2  B0006VJ6TO                                    Body Back Buddy   \n",
       "3  B000EZSFXA         SnackMasters California Style Turkey Jerky   \n",
       "4  B000KV61FC  Premier Busy Buddy Tug-a-Jug Treat Dispensing ...   \n",
       "\n",
       "           userId        Time  \\\n",
       "0  A2L6QTQQI13LZG  1344211200   \n",
       "1   A2HJUOZ9R9K4F  1344211200   \n",
       "2  A14PK96LL78NN3  1344211200   \n",
       "3  A2UW73HU9UMOTY  1344211200   \n",
       "4  A1Q99RNV0TKW8R  1344211200   \n",
       "\n",
       "                                                Text                  Cat1  \\\n",
       "0  We've only had it installed about 2 weeks. So ...          pet supplies   \n",
       "1  My bunny had a hard time eating this because t...          pet supplies   \n",
       "2  would never in a million years have guessed th...  health personal care   \n",
       "3  Being the jerky fanatic I am, snackmasters han...  grocery gourmet food   \n",
       "4  Wondered how quick my dog would catch on to th...          pet supplies   \n",
       "\n",
       "                   Cat2                Cat3  \n",
       "0                  cats           cat flaps  \n",
       "1  bunny rabbit central                food  \n",
       "2           health care  massage relaxation  \n",
       "3            snack food   jerky dried meats  \n",
       "4                  dogs                toys  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   productId  10000 non-null  object\n",
      " 1   Title      9995 non-null   object\n",
      " 2   userId     10000 non-null  object\n",
      " 3   Time       10000 non-null  int64 \n",
      " 4   Text       10000 non-null  object\n",
      " 5   Cat1       10000 non-null  object\n",
      " 6   Cat2       10000 non-null  object\n",
      " 7   Cat3       10000 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 625.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "productId    0\n",
       "Title        5\n",
       "userId       0\n",
       "Time         0\n",
       "Text         0\n",
       "Cat1         0\n",
       "Cat2         0\n",
       "Cat3         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head())\n",
    "display(data.info())\n",
    "display(data.isnull().sum())\n",
    "data.dropna(inplace=True)  # Drop rows with missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the text data\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "# Preprocess both Title and Text columns and combine them\n",
    "def preprocess_data(data):\n",
    "    data['clean_text'] = data['Text'].apply(preprocess_text)\n",
    "    data['clean_title'] = data['Title'].apply(preprocess_text)\n",
    "    data['combined_text'] = data['clean_title'] + \" \" + data['clean_text']\n",
    "    \n",
    "    # Encode target labels\n",
    "    le_cat1 = LabelEncoder()\n",
    "    le_cat2 = LabelEncoder()\n",
    "    le_cat3 = LabelEncoder()\n",
    "    data['Cat1'] = le_cat1.fit_transform(data['Cat1'])\n",
    "    data['Cat2'] = le_cat2.fit_transform(data['Cat2'])\n",
    "    data['Cat3'] = le_cat3.fit_transform(data['Cat3'])\n",
    "    \n",
    "    return data, le_cat1, le_cat2, le_cat3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "def split_data(data):\n",
    "    X = data['combined_text']\n",
    "    y_cat1 = data['Cat1']\n",
    "    y_cat2 = data['Cat2']\n",
    "    y_cat3 = data['Cat3']\n",
    "    \n",
    "    X_train, X_test, y_train_cat1, y_test_cat1 = train_test_split(X, y_cat1, test_size=0.2, random_state=42)\n",
    "    X_train_cat2, X_test_cat2, y_train_cat2, y_test_cat2 = train_test_split(X, y_cat2, test_size=0.2, random_state=42)\n",
    "    X_train_cat3, X_test_cat3, y_train_cat3, y_test_cat3 = train_test_split(X, y_cat3, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return (X_train, X_test, y_train_cat1, y_test_cat1,\n",
    "            X_train_cat2, X_test_cat2, y_train_cat2, y_test_cat2,\n",
    "            X_train_cat3, X_test_cat3, y_train_cat3, y_test_cat3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TF-IDF Vectorization**\n",
    "\n",
    "- **TF-IDF** stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). TF-IDF is used as a weighting factor in text mining and information retrieval.\n",
    "\n",
    "##### Components of TF-IDF\n",
    "\n",
    "1. **Term Frequency (TF)**: Measures how frequently a term appears in a document.\n",
    "   - \\( \\text{TF}(t,d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} \\)\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: Measures how important a term is across the corpus.\n",
    "   - \\( \\text{IDF}(t,D) = \\log \\left( \\frac{\\text{Total number of documents } N}{\\text{Number of documents containing term } t} \\right) \\)\n",
    "   - Terms that are common in many documents will have a lower IDF score.\n",
    "\n",
    "3. **TF-IDF Score**: The product of TF and IDF.\n",
    "   - \\( \\text{TF-IDF}(t,d,D) = \\text{TF}(t,d) \\times \\text{IDF}(t,D) \\)\n",
    "\n",
    "##### Why Use TF-IDF?\n",
    "\n",
    "- **Relevance**: TF-IDF helps to highlight important words in the documents by considering their frequency in a particular document relative to the entire corpus. Common words (like \"the\", \"is\", \"and\") are down-weighted.\n",
    "- **Feature Representation**: Converts text data into a numerical representation that can be fed into machine learning algorithms.\n",
    "- **Discriminative Power**: It improves the discriminative power of text classification models by focusing on unique and meaningful terms.\n",
    "\n",
    "##### Why TF-IDF is Preferred?\n",
    "\n",
    "- **Effective Weighting**: Unlike simple term frequency, TF-IDF considers the distribution of terms across all documents, which helps in distinguishing between relevant and non-relevant terms.\n",
    "- **Dimensionality Reduction**: It can reduce the dimensionality of the feature space by focusing on the most significant terms.\n",
    "- **Interpretability**: The resulting weights provide insight into the importance of terms within documents.\n",
    "\n",
    "##### Parameter: `max_features`\n",
    "\n",
    "- **Definition**: `max_features` specifies the maximum number of features (terms) to consider when building the TF-IDF matrix.\n",
    "- **Usage**: In this case, `max_features=5000` means that the top 5000 terms with the highest TF-IDF scores will be selected as features for the model.\n",
    "- **Benefits**:\n",
    "  - **Computational Efficiency**: Reduces the size of the feature space, making the model more efficient to train and less prone to overfitting.\n",
    "  - **Focus on Most Important Terms**: Ensures that only the most relevant terms (with the highest TF-IDF scores) are used as features, improving the performance and interpretability of the model.\n",
    "\n",
    "##### Summary\n",
    "\n",
    "- **TF-IDF**: A statistical measure used to evaluate the importance of a word in a document relative to a collection of documents.\n",
    "- **Benefits**: Improves the relevance and discriminative power of text features.\n",
    "- **max_features**: Limits the number of features to the most significant 5000 terms, enhancing computational efficiency and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build and evaluate models\n",
    "def build_and_evaluate_model(X_train, X_test, y_train, y_test, classifiers, param_grids):\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "    best_clf_name = \"\"\n",
    "    \n",
    "    for clf_name, clf, param_grid in zip(classifiers.keys(), classifiers.values(), param_grids.values()):\n",
    "        pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "            ('clf', clf)\n",
    "        ])\n",
    "        \n",
    "        # Base model evaluation\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        predictions = pipeline.predict(X_test)\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        print(f\"Base Model Classification Report for {clf_name}\")\n",
    "        print(classification_report(y_test, predictions))\n",
    "        print(f\"Base Model F1 Score for {clf_name}: {f1}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = pipeline\n",
    "            best_clf_name = clf_name\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_clf = grid_search.best_estimator_\n",
    "        predictions = best_clf.predict(X_test)\n",
    "        \n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = best_clf\n",
    "            best_clf_name = clf_name\n",
    "        \n",
    "        print(f\"Tuned Model Classification Report for {clf_name}\")\n",
    "        print(classification_report(y_test, predictions))\n",
    "        print(f\"Tuned Model F1 Score for {clf_name}: {f1}\")\n",
    "    \n",
    "    print(f\"\\nBest model: {best_clf_name} with F1 Score: {best_f1}\")\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Function Calling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's go through the hyperparameters used in the `param_grids` dictionary for each classifier in detail.\n",
    "\n",
    "#### **Logistic Regression (`LogisticRegression`)**\n",
    "\n",
    "1. **`C`**: Inverse of regularization strength (float, default=1.0)\n",
    "   - Smaller values specify stronger regularization.\n",
    "   - Controls the trade-off between achieving a low training error and a low testing error.\n",
    "\n",
    "2. **`solver`**: Algorithm to use in the optimization problem (string, default='lbfgs')\n",
    "   - `'liblinear'`: A good choice for small datasets. Uses a coordinate descent algorithm.\n",
    "\n",
    "#### **Multinomial Naive Bayes (`MultinomialNB`)**\n",
    "\n",
    "1. **`alpha`**: Additive (Laplace/Lidstone) smoothing parameter (float, default=1.0)\n",
    "   - 0 for no smoothing.\n",
    "   - Higher values of alpha result in smoother probability estimates.\n",
    "\n",
    "#### **Support Vector Classifier (`SVC`)**\n",
    "\n",
    "1. **`C`**: Regularization parameter (float, default=1.0)\n",
    "   - The strength of the regularization is inversely proportional to `C`.\n",
    "   - Must be strictly positive.\n",
    "   - Smaller values specify stronger regularization.\n",
    "\n",
    "2. **`kernel`**: Specifies the kernel type to be used in the algorithm (string, default='rbf')\n",
    "   - `'linear'`: Linear kernel.\n",
    "   - `'rbf'`: Radial basis function (Gaussian) kernel.\n",
    "   - The kernel defines the decision boundary shape in the feature space.\n",
    "\n",
    "#### **Random Forest Classifier (`RandomForestClassifier`)**\n",
    "\n",
    "1. **`n_estimators`**: The number of trees in the forest (int, default=100)\n",
    "   - Higher values typically improve performance but increase computation time.\n",
    "\n",
    "2. **`max_depth`**: The maximum depth of the tree (int, default=None)\n",
    "   - Limits the depth of the tree to prevent overfitting.\n",
    "   - Higher values increase the risk of overfitting.\n",
    "\n",
    "#### **Decision Tree Classifier (`DecisionTreeClassifier`)**\n",
    "\n",
    "1. **`max_depth`**: The maximum depth of the tree (int, default=None)\n",
    "   - Limits the depth of the tree to prevent overfitting.\n",
    "   - Higher values increase the risk of overfitting.\n",
    "\n",
    "2. **`min_samples_split`**: The minimum number of samples required to split an internal node (int or float, default=2)\n",
    "   - Higher values can prevent the model from learning overly specific patterns and help prevent overfitting.\n",
    "\n",
    "\n",
    "#### **Summary**\n",
    "- **Regularization Parameters (`C`, `alpha`)**: Control the trade-off between achieving a low training error and a low testing error.\n",
    "- **Solver (`solver`)**: Determines the optimization algorithm for Logistic Regression.\n",
    "- **Kernel (`kernel`)**: Defines the decision boundary shape for SVC.\n",
    "- **Number of Estimators (`n_estimators`)**: Determines the number of trees (or boosting rounds) in ensemble methods.\n",
    "- **Tree Depth (`max_depth`)**: Limits the complexity of the model to prevent overfitting.\n",
    "- **Minimum Samples Split (`min_samples_split`)**: Ensures nodes have enough samples before splitting to prevent overly specific patterns.\n",
    "\n",
    "These parameters are crucial for tuning the model to achieve the best performance by balancing bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating models for Cat1\n",
      "Base Model Classification Report for LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.67      0.77       135\n",
      "           1       0.92      0.88      0.90       438\n",
      "           2       0.96      0.76      0.85       138\n",
      "           3       0.78      0.94      0.86       601\n",
      "           4       0.97      0.87      0.92       313\n",
      "           5       0.90      0.88      0.89       374\n",
      "\n",
      "    accuracy                           0.88      1999\n",
      "   macro avg       0.91      0.83      0.86      1999\n",
      "weighted avg       0.89      0.88      0.88      1999\n",
      "\n",
      "Base Model F1 Score for LogisticRegression: 0.8754163207407083\n",
      "Tuned Model Classification Report for LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85       135\n",
      "           1       0.91      0.90      0.90       438\n",
      "           2       0.95      0.83      0.88       138\n",
      "           3       0.85      0.93      0.88       601\n",
      "           4       0.96      0.93      0.94       313\n",
      "           5       0.92      0.90      0.91       374\n",
      "\n",
      "    accuracy                           0.90      1999\n",
      "   macro avg       0.91      0.88      0.90      1999\n",
      "weighted avg       0.90      0.90      0.90      1999\n",
      "\n",
      "Tuned Model F1 Score for LogisticRegression: 0.9005439333085757\n",
      "Base Model Classification Report for MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.41      0.59       135\n",
      "           1       0.89      0.88      0.89       438\n",
      "           2       0.99      0.54      0.69       138\n",
      "           3       0.75      0.95      0.84       601\n",
      "           4       0.95      0.86      0.90       313\n",
      "           5       0.87      0.91      0.89       374\n",
      "\n",
      "    accuracy                           0.85      1999\n",
      "   macro avg       0.91      0.76      0.80      1999\n",
      "weighted avg       0.87      0.85      0.84      1999\n",
      "\n",
      "Base Model F1 Score for MultinomialNB: 0.8424611204003241\n",
      "Tuned Model Classification Report for MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75       135\n",
      "           1       0.90      0.89      0.89       438\n",
      "           2       0.96      0.71      0.82       138\n",
      "           3       0.81      0.94      0.87       601\n",
      "           4       0.94      0.90      0.92       313\n",
      "           5       0.90      0.93      0.91       374\n",
      "\n",
      "    accuracy                           0.88      1999\n",
      "   macro avg       0.92      0.83      0.86      1999\n",
      "weighted avg       0.89      0.88      0.88      1999\n",
      "\n",
      "Tuned Model F1 Score for MultinomialNB: 0.878969484115894\n",
      "Base Model Classification Report for SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.67      0.78       135\n",
      "           1       0.93      0.87      0.90       438\n",
      "           2       1.00      0.73      0.85       138\n",
      "           3       0.77      0.95      0.85       601\n",
      "           4       0.97      0.87      0.92       313\n",
      "           5       0.91      0.89      0.90       374\n",
      "\n",
      "    accuracy                           0.88      1999\n",
      "   macro avg       0.92      0.83      0.87      1999\n",
      "weighted avg       0.89      0.88      0.88      1999\n",
      "\n",
      "Base Model F1 Score for SVC: 0.8767859939602277\n",
      "Tuned Model Classification Report for SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85       135\n",
      "           1       0.91      0.89      0.90       438\n",
      "           2       0.97      0.84      0.90       138\n",
      "           3       0.83      0.93      0.88       601\n",
      "           4       0.97      0.90      0.93       313\n",
      "           5       0.91      0.90      0.91       374\n",
      "\n",
      "    accuracy                           0.90      1999\n",
      "   macro avg       0.92      0.88      0.90      1999\n",
      "weighted avg       0.90      0.90      0.90      1999\n",
      "\n",
      "Tuned Model F1 Score for SVC: 0.8972561261439357\n",
      "Base Model Classification Report for RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.52      0.66       135\n",
      "           1       0.90      0.86      0.88       438\n",
      "           2       0.88      0.70      0.78       138\n",
      "           3       0.74      0.86      0.80       601\n",
      "           4       0.92      0.88      0.90       313\n",
      "           5       0.81      0.86      0.83       374\n",
      "\n",
      "    accuracy                           0.83      1999\n",
      "   macro avg       0.86      0.78      0.81      1999\n",
      "weighted avg       0.84      0.83      0.83      1999\n",
      "\n",
      "Base Model F1 Score for RandomForest: 0.827869323809941\n",
      "Tuned Model Classification Report for RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.32      0.48       135\n",
      "           1       0.90      0.76      0.83       438\n",
      "           2       1.00      0.41      0.58       138\n",
      "           3       0.60      0.93      0.73       601\n",
      "           4       0.92      0.81      0.86       313\n",
      "           5       0.85      0.74      0.79       374\n",
      "\n",
      "    accuracy                           0.76      1999\n",
      "   macro avg       0.87      0.66      0.71      1999\n",
      "weighted avg       0.81      0.76      0.76      1999\n",
      "\n",
      "Tuned Model F1 Score for RandomForest: 0.7555007239632137\n",
      "Base Model Classification Report for DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.57       135\n",
      "           1       0.81      0.73      0.77       438\n",
      "           2       0.59      0.62      0.61       138\n",
      "           3       0.68      0.69      0.68       601\n",
      "           4       0.83      0.85      0.84       313\n",
      "           5       0.73      0.74      0.74       374\n",
      "\n",
      "    accuracy                           0.72      1999\n",
      "   macro avg       0.70      0.70      0.70      1999\n",
      "weighted avg       0.72      0.72      0.72      1999\n",
      "\n",
      "Base Model F1 Score for DecisionTree: 0.7231399913829937\n",
      "Tuned Model Classification Report for DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.41      0.50       135\n",
      "           1       0.89      0.61      0.72       438\n",
      "           2       0.73      0.44      0.55       138\n",
      "           3       0.55      0.86      0.67       601\n",
      "           4       0.90      0.80      0.84       313\n",
      "           5       0.77      0.63      0.69       374\n",
      "\n",
      "    accuracy                           0.69      1999\n",
      "   macro avg       0.74      0.63      0.66      1999\n",
      "weighted avg       0.74      0.69      0.69      1999\n",
      "\n",
      "Tuned Model F1 Score for DecisionTree: 0.6931880905602403\n",
      "\n",
      "Best model: LogisticRegression with F1 Score: 0.9005439333085757\n",
      "\n",
      "Training and evaluating models for Cat2\n",
      "Base Model Classification Report for LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.38      0.51        24\n",
      "           1       0.90      0.35      0.50        26\n",
      "           2       1.00      0.42      0.59        12\n",
      "           4       0.59      0.56      0.57        36\n",
      "           5       0.85      0.37      0.51        30\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.70      0.68      0.69        31\n",
      "           8       0.75      0.30      0.43        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       0.72      0.86      0.78        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.92      0.44      0.59        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.77      0.70      0.73        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.96      0.96      0.96        23\n",
      "          19       0.72      0.89      0.80       178\n",
      "          20       0.75      0.67      0.71         9\n",
      "          21       0.69      0.61      0.65        18\n",
      "          22       0.71      0.36      0.48        14\n",
      "          23       1.00      0.38      0.55        29\n",
      "          24       0.95      0.62      0.75        34\n",
      "          25       0.97      1.00      0.99       107\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.77      0.80      0.78        59\n",
      "          28       0.69      0.69      0.69        13\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       0.00      0.00      0.00         8\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.84      0.88      0.86       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.38      0.79      0.51       123\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       0.60      0.19      0.29        16\n",
      "          37       0.77      0.76      0.76        90\n",
      "          38       0.59      0.35      0.44        37\n",
      "          39       0.85      0.88      0.86        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.78      0.83      0.80        82\n",
      "          43       0.00      0.00      0.00        13\n",
      "          44       0.65      0.62      0.63        21\n",
      "          45       0.58      0.92      0.71       180\n",
      "          46       0.60      0.62      0.61        40\n",
      "          47       0.68      0.76      0.72        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       1.00      0.33      0.50         3\n",
      "          51       0.67      0.50      0.57        12\n",
      "          52       1.00      0.38      0.55        21\n",
      "          54       0.83      0.28      0.42        18\n",
      "          55       0.74      0.78      0.76       102\n",
      "          57       1.00      0.07      0.12        15\n",
      "          58       0.00      0.00      0.00        16\n",
      "          59       0.00      0.00      0.00         5\n",
      "          60       0.91      0.56      0.69        36\n",
      "          61       1.00      0.28      0.43        18\n",
      "          62       1.00      0.43      0.60         7\n",
      "          63       0.77      0.59      0.67        17\n",
      "\n",
      "    accuracy                           0.70      1999\n",
      "   macro avg       0.57      0.41      0.45      1999\n",
      "weighted avg       0.71      0.70      0.68      1999\n",
      "\n",
      "Base Model F1 Score for LogisticRegression: 0.6760926139662172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.54      0.65        24\n",
      "           1       0.78      0.54      0.64        26\n",
      "           2       0.89      0.67      0.76        12\n",
      "           4       0.63      0.53      0.58        36\n",
      "           5       0.74      0.47      0.57        30\n",
      "           6       0.80      0.57      0.67         7\n",
      "           7       0.81      0.84      0.83        31\n",
      "           8       0.71      0.50      0.59        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       1.00      0.60      0.75         5\n",
      "          11       0.82      0.86      0.84        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.85      0.68      0.76        25\n",
      "          14       1.00      1.00      1.00         1\n",
      "          15       0.75      0.70      0.72        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.96      0.96      0.96        23\n",
      "          19       0.80      0.88      0.84       178\n",
      "          20       1.00      0.67      0.80         9\n",
      "          21       0.64      0.78      0.70        18\n",
      "          22       0.56      0.64      0.60        14\n",
      "          23       0.76      0.66      0.70        29\n",
      "          24       0.93      0.82      0.88        34\n",
      "          25       0.94      0.98      0.96       107\n",
      "          26       1.00      1.00      1.00         1\n",
      "          27       0.80      0.81      0.81        59\n",
      "          28       0.62      0.77      0.69        13\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      0.50      0.67         8\n",
      "          31       1.00      0.15      0.27        13\n",
      "          32       0.89      0.89      0.89       105\n",
      "          33       0.50      0.33      0.40         3\n",
      "          34       0.51      0.75      0.61       123\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       0.55      0.38      0.44        16\n",
      "          37       0.76      0.81      0.78        90\n",
      "          38       0.54      0.41      0.46        37\n",
      "          39       0.86      0.92      0.89        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.77      0.90      0.83        82\n",
      "          43       0.43      0.46      0.44        13\n",
      "          44       0.63      0.81      0.71        21\n",
      "          45       0.75      0.92      0.82       180\n",
      "          46       0.60      0.70      0.64        40\n",
      "          47       0.78      0.82      0.80        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       1.00      0.33      0.50         3\n",
      "          51       0.80      0.67      0.73        12\n",
      "          52       1.00      0.57      0.73        21\n",
      "          54       0.75      0.50      0.60        18\n",
      "          55       0.76      0.79      0.78       102\n",
      "          57       0.75      0.40      0.52        15\n",
      "          58       0.67      0.12      0.21        16\n",
      "          59       0.50      0.40      0.44         5\n",
      "          60       0.82      0.64      0.72        36\n",
      "          61       0.86      0.33      0.48        18\n",
      "          62       1.00      0.57      0.73         7\n",
      "          63       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.76      1999\n",
      "   macro avg       0.70      0.57      0.61      1999\n",
      "weighted avg       0.76      0.76      0.75      1999\n",
      "\n",
      "Tuned Model F1 Score for LogisticRegression: 0.7490151819744962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.00      0.00      0.00        26\n",
      "           2       0.00      0.00      0.00        12\n",
      "           4       0.67      0.06      0.10        36\n",
      "           5       1.00      0.03      0.06        30\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       1.00      0.23      0.37        31\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       1.00      0.19      0.32        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.88      0.48      0.62        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.83      0.90        23\n",
      "          19       0.28      0.97      0.44       178\n",
      "          20       0.00      0.00      0.00         9\n",
      "          21       0.50      0.06      0.10        18\n",
      "          22       1.00      0.07      0.13        14\n",
      "          23       0.00      0.00      0.00        29\n",
      "          24       1.00      0.12      0.21        34\n",
      "          25       0.84      1.00      0.91       107\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.85      0.47      0.61        59\n",
      "          28       0.00      0.00      0.00        13\n",
      "          29       0.00      0.00      0.00         5\n",
      "          30       0.00      0.00      0.00         8\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.82      0.90      0.85       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.41      0.67      0.51       123\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       1.00      0.12      0.22        16\n",
      "          37       0.89      0.57      0.69        90\n",
      "          38       0.57      0.11      0.18        37\n",
      "          39       0.89      0.76      0.82        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.93      0.70      0.80        82\n",
      "          43       0.00      0.00      0.00        13\n",
      "          44       1.00      0.10      0.17        21\n",
      "          45       0.41      0.94      0.57       180\n",
      "          46       0.88      0.17      0.29        40\n",
      "          47       0.74      0.73      0.74        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00        12\n",
      "          52       0.00      0.00      0.00        21\n",
      "          54       0.00      0.00      0.00        18\n",
      "          55       0.66      0.70      0.68       102\n",
      "          57       0.00      0.00      0.00        15\n",
      "          58       0.00      0.00      0.00        16\n",
      "          59       0.00      0.00      0.00         5\n",
      "          60       0.75      0.08      0.15        36\n",
      "          61       0.00      0.00      0.00        18\n",
      "          62       0.00      0.00      0.00         7\n",
      "          63       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.53      1999\n",
      "   macro avg       0.33      0.18      0.19      1999\n",
      "weighted avg       0.57      0.53      0.47      1999\n",
      "\n",
      "Base Model F1 Score for MultinomialNB: 0.4698196098076801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.29        24\n",
      "           1       0.00      0.00      0.00        26\n",
      "           2       0.00      0.00      0.00        12\n",
      "           4       0.77      0.28      0.41        36\n",
      "           5       1.00      0.13      0.24        30\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.88      0.48      0.62        31\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       1.00      0.38      0.55        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      0.04      0.08        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.70      0.54      0.61        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.87      0.93        23\n",
      "          19       0.36      0.95      0.53       178\n",
      "          20       1.00      0.33      0.50         9\n",
      "          21       0.71      0.28      0.40        18\n",
      "          22       1.00      0.07      0.13        14\n",
      "          23       1.00      0.07      0.13        29\n",
      "          24       1.00      0.26      0.42        34\n",
      "          25       0.80      1.00      0.89       107\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.79      0.64      0.71        59\n",
      "          28       1.00      0.15      0.27        13\n",
      "          29       0.00      0.00      0.00         5\n",
      "          30       0.00      0.00      0.00         8\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.83      0.90      0.86       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.40      0.72      0.51       123\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       0.67      0.12      0.21        16\n",
      "          37       0.88      0.71      0.79        90\n",
      "          38       0.53      0.24      0.33        37\n",
      "          39       0.86      0.86      0.86        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.83      0.79      0.81        82\n",
      "          43       0.00      0.00      0.00        13\n",
      "          44       0.83      0.24      0.37        21\n",
      "          45       0.50      0.94      0.65       180\n",
      "          46       0.81      0.42      0.56        40\n",
      "          47       0.70      0.77      0.74        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.00      0.00      0.00         3\n",
      "          51       0.67      0.33      0.44        12\n",
      "          52       0.00      0.00      0.00        21\n",
      "          54       0.00      0.00      0.00        18\n",
      "          55       0.69      0.75      0.72       102\n",
      "          57       0.00      0.00      0.00        15\n",
      "          58       0.00      0.00      0.00        16\n",
      "          59       0.00      0.00      0.00         5\n",
      "          60       0.69      0.25      0.37        36\n",
      "          61       1.00      0.17      0.29        18\n",
      "          62       0.00      0.00      0.00         7\n",
      "          63       0.80      0.24      0.36        17\n",
      "\n",
      "    accuracy                           0.60      1999\n",
      "   macro avg       0.45      0.25      0.28      1999\n",
      "weighted avg       0.64      0.60      0.55      1999\n",
      "\n",
      "Tuned Model F1 Score for MultinomialNB: 0.5487496319562111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.33      0.48        24\n",
      "           1       1.00      0.35      0.51        26\n",
      "           2       1.00      0.50      0.67        12\n",
      "           4       0.62      0.44      0.52        36\n",
      "           5       0.92      0.37      0.52        30\n",
      "           6       1.00      0.14      0.25         7\n",
      "           7       0.82      0.74      0.78        31\n",
      "           8       0.75      0.30      0.43        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       1.00      0.40      0.57         5\n",
      "          11       0.86      0.90      0.88        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       1.00      0.48      0.65        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.81      0.67      0.73        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       1.00      0.96      0.98        23\n",
      "          19       0.78      0.88      0.83       178\n",
      "          20       0.86      0.67      0.75         9\n",
      "          21       0.57      0.44      0.50        18\n",
      "          22       0.71      0.36      0.48        14\n",
      "          23       1.00      0.34      0.51        29\n",
      "          24       1.00      0.56      0.72        34\n",
      "          25       0.98      0.98      0.98       107\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.90      0.80      0.85        59\n",
      "          28       0.82      0.69      0.75        13\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       0.00      0.00      0.00         8\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.87      0.87      0.87       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.26      0.82      0.40       123\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       0.67      0.25      0.36        16\n",
      "          37       0.82      0.68      0.74        90\n",
      "          38       0.56      0.41      0.47        37\n",
      "          39       0.93      0.87      0.90        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.94      0.83      0.88        82\n",
      "          43       0.00      0.00      0.00        13\n",
      "          44       0.67      0.57      0.62        21\n",
      "          45       0.57      0.93      0.71       180\n",
      "          46       0.70      0.57      0.63        40\n",
      "          47       0.78      0.75      0.77        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       1.00      0.33      0.50         3\n",
      "          51       1.00      0.42      0.59        12\n",
      "          52       1.00      0.24      0.38        21\n",
      "          54       0.75      0.17      0.27        18\n",
      "          55       0.76      0.78      0.77       102\n",
      "          57       1.00      0.07      0.12        15\n",
      "          58       0.00      0.00      0.00        16\n",
      "          59       0.50      0.20      0.29         5\n",
      "          60       0.91      0.58      0.71        36\n",
      "          61       1.00      0.28      0.43        18\n",
      "          62       1.00      0.43      0.60         7\n",
      "          63       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.69      1999\n",
      "   macro avg       0.65      0.42      0.47      1999\n",
      "weighted avg       0.76      0.69      0.68      1999\n",
      "\n",
      "Base Model F1 Score for SVC: 0.6823069319625384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.46      0.56        24\n",
      "           1       0.71      0.58      0.64        26\n",
      "           2       1.00      0.67      0.80        12\n",
      "           4       0.62      0.56      0.59        36\n",
      "           5       0.74      0.57      0.64        30\n",
      "           6       0.50      0.57      0.53         7\n",
      "           7       0.84      0.87      0.86        31\n",
      "           8       0.86      0.60      0.71        10\n",
      "           9       1.00      0.25      0.40         4\n",
      "          10       0.57      0.80      0.67         5\n",
      "          11       0.82      0.86      0.84        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.80      0.64      0.71        25\n",
      "          14       0.50      1.00      0.67         1\n",
      "          15       0.68      0.74      0.71        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       1.00      0.50      0.67         2\n",
      "          18       1.00      0.96      0.98        23\n",
      "          19       0.84      0.83      0.83       178\n",
      "          20       1.00      0.78      0.88         9\n",
      "          21       0.67      0.78      0.72        18\n",
      "          22       0.45      0.64      0.53        14\n",
      "          23       0.70      0.66      0.68        29\n",
      "          24       0.93      0.79      0.86        34\n",
      "          25       0.96      0.96      0.96       107\n",
      "          26       1.00      1.00      1.00         1\n",
      "          27       0.78      0.80      0.79        59\n",
      "          28       0.73      0.85      0.79        13\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      0.50      0.67         8\n",
      "          31       0.50      0.08      0.13        13\n",
      "          32       0.88      0.90      0.89       105\n",
      "          33       0.50      0.33      0.40         3\n",
      "          34       0.47      0.72      0.57       123\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       0.55      0.38      0.44        16\n",
      "          37       0.83      0.79      0.81        90\n",
      "          38       0.61      0.46      0.52        37\n",
      "          39       0.90      0.91      0.90        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.70      0.89      0.78        82\n",
      "          43       0.38      0.38      0.38        13\n",
      "          44       0.68      0.81      0.74        21\n",
      "          45       0.73      0.87      0.79       180\n",
      "          46       0.65      0.65      0.65        40\n",
      "          47       0.78      0.81      0.80        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       1.00      1.00      1.00         3\n",
      "          51       0.82      0.75      0.78        12\n",
      "          52       1.00      0.57      0.73        21\n",
      "          54       0.77      0.56      0.65        18\n",
      "          55       0.76      0.76      0.76       102\n",
      "          57       0.86      0.40      0.55        15\n",
      "          58       1.00      0.12      0.22        16\n",
      "          59       0.67      0.40      0.50         5\n",
      "          60       0.89      0.67      0.76        36\n",
      "          61       0.86      0.33      0.48        18\n",
      "          62       1.00      0.57      0.73         7\n",
      "          63       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.75      1999\n",
      "   macro avg       0.72      0.60      0.63      1999\n",
      "weighted avg       0.77      0.75      0.75      1999\n",
      "\n",
      "Tuned Model F1 Score for SVC: 0.7467026896013591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.38      0.43        24\n",
      "           1       1.00      0.31      0.47        26\n",
      "           2       0.88      0.58      0.70        12\n",
      "           4       0.33      0.31      0.32        36\n",
      "           5       0.83      0.33      0.48        30\n",
      "           6       0.33      0.14      0.20         7\n",
      "           7       0.65      0.84      0.73        31\n",
      "           8       0.62      0.50      0.56        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       1.00      0.60      0.75         5\n",
      "          11       0.61      0.90      0.73        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.79      0.60      0.68        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.83      0.71      0.77        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.88      0.96      0.92        23\n",
      "          19       0.79      0.89      0.83       178\n",
      "          20       0.80      0.89      0.84         9\n",
      "          21       0.27      0.33      0.30        18\n",
      "          22       0.75      0.43      0.55        14\n",
      "          23       0.92      0.38      0.54        29\n",
      "          24       0.85      0.82      0.84        34\n",
      "          25       0.92      0.98      0.95       107\n",
      "          26       1.00      1.00      1.00         1\n",
      "          27       0.69      0.86      0.77        59\n",
      "          28       0.75      0.69      0.72        13\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      0.12      0.22         8\n",
      "          31       0.20      0.08      0.11        13\n",
      "          32       0.72      0.88      0.79       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.42      0.57      0.48       123\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       0.60      0.38      0.46        16\n",
      "          37       0.75      0.71      0.73        90\n",
      "          38       0.74      0.46      0.57        37\n",
      "          39       0.78      0.88      0.83        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.70      0.79      0.74        82\n",
      "          43       0.67      0.31      0.42        13\n",
      "          44       0.46      0.76      0.57        21\n",
      "          45       0.65      0.87      0.75       180\n",
      "          46       0.52      0.57      0.55        40\n",
      "          47       0.86      0.71      0.78        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       1.00      0.67      0.80         3\n",
      "          51       0.62      0.83      0.71        12\n",
      "          52       1.00      0.57      0.73        21\n",
      "          54       0.70      0.39      0.50        18\n",
      "          55       0.68      0.82      0.75       102\n",
      "          57       1.00      0.13      0.24        15\n",
      "          58       0.50      0.12      0.20        16\n",
      "          59       0.50      0.40      0.44         5\n",
      "          60       0.84      0.58      0.69        36\n",
      "          61       0.83      0.28      0.42        18\n",
      "          62       0.75      0.43      0.55         7\n",
      "          63       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.70      1999\n",
      "   macro avg       0.62      0.49      0.52      1999\n",
      "weighted avg       0.71      0.70      0.69      1999\n",
      "\n",
      "Base Model F1 Score for RandomForest: 0.6854137782651998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.21      0.33        24\n",
      "           1       0.00      0.00      0.00        26\n",
      "           2       0.86      0.50      0.63        12\n",
      "           4       0.73      0.22      0.34        36\n",
      "           5       0.82      0.30      0.44        30\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.59      0.77      0.67        31\n",
      "           8       0.00      0.00      0.00        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       1.00      0.20      0.33         5\n",
      "          11       1.00      0.62      0.76        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.86      0.24      0.38        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.85      0.70      0.77        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.88      0.96      0.92        23\n",
      "          19       0.64      0.91      0.75       178\n",
      "          20       0.75      0.67      0.71         9\n",
      "          21       0.50      0.17      0.25        18\n",
      "          22       0.00      0.00      0.00        14\n",
      "          23       1.00      0.03      0.07        29\n",
      "          24       0.83      0.59      0.69        34\n",
      "          25       0.86      0.99      0.92       107\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.82      0.68      0.74        59\n",
      "          28       1.00      0.15      0.27        13\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       0.00      0.00      0.00         8\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.72      0.86      0.78       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.31      0.32      0.32       123\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       1.00      0.19      0.32        16\n",
      "          37       0.82      0.47      0.60        90\n",
      "          38       0.55      0.16      0.25        37\n",
      "          39       0.75      0.70      0.72        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.87      0.59      0.70        82\n",
      "          43       0.00      0.00      0.00        13\n",
      "          44       0.83      0.24      0.37        21\n",
      "          45       0.25      0.93      0.39       180\n",
      "          46       0.58      0.17      0.27        40\n",
      "          47       0.90      0.55      0.68        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.00      0.00      0.00         3\n",
      "          51       0.56      0.75      0.64        12\n",
      "          52       0.00      0.00      0.00        21\n",
      "          54       0.00      0.00      0.00        18\n",
      "          55       0.72      0.76      0.74       102\n",
      "          57       1.00      0.07      0.12        15\n",
      "          58       0.00      0.00      0.00        16\n",
      "          59       0.00      0.00      0.00         5\n",
      "          60       0.75      0.33      0.46        36\n",
      "          61       0.00      0.00      0.00        18\n",
      "          62       1.00      0.43      0.60         7\n",
      "          63       1.00      0.18      0.30        17\n",
      "\n",
      "    accuracy                           0.56      1999\n",
      "   macro avg       0.47      0.29      0.32      1999\n",
      "weighted avg       0.63      0.56      0.53      1999\n",
      "\n",
      "Tuned Model F1 Score for RandomForest: 0.5332641124280976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.21      0.24        24\n",
      "           1       0.31      0.31      0.31        26\n",
      "           2       0.70      0.58      0.64        12\n",
      "           4       0.33      0.25      0.29        36\n",
      "           5       0.60      0.30      0.40        30\n",
      "           6       0.17      0.14      0.15         7\n",
      "           7       0.81      0.81      0.81        31\n",
      "           8       0.36      0.50      0.42        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.67      0.40      0.50         5\n",
      "          11       0.58      0.86      0.69        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.67      0.48      0.56        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.69      0.61      0.65        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.50      0.50      0.50         2\n",
      "          18       0.87      0.87      0.87        23\n",
      "          19       0.75      0.79      0.77       178\n",
      "          20       0.41      0.78      0.54         9\n",
      "          21       0.17      0.17      0.17        18\n",
      "          22       0.33      0.36      0.34        14\n",
      "          23       0.41      0.24      0.30        29\n",
      "          24       0.81      0.65      0.72        34\n",
      "          25       0.90      0.94      0.92       107\n",
      "          26       1.00      1.00      1.00         1\n",
      "          27       0.66      0.71      0.68        59\n",
      "          28       0.21      0.23      0.22        13\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      0.75      0.86         8\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.75      0.82      0.79       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.37      0.40      0.38       123\n",
      "          35       0.33      0.50      0.40         2\n",
      "          36       0.22      0.25      0.24        16\n",
      "          37       0.57      0.66      0.61        90\n",
      "          38       0.50      0.46      0.48        37\n",
      "          39       0.70      0.72      0.71        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.68      0.76      0.72        82\n",
      "          43       0.12      0.15      0.13        13\n",
      "          44       0.41      0.52      0.46        21\n",
      "          45       0.62      0.68      0.65       180\n",
      "          46       0.35      0.33      0.34        40\n",
      "          47       0.58      0.64      0.61        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.67      0.67      0.67         3\n",
      "          50       0.00      0.00      0.00         0\n",
      "          51       0.64      0.58      0.61        12\n",
      "          52       0.73      0.52      0.61        21\n",
      "          54       0.50      0.44      0.47        18\n",
      "          55       0.51      0.56      0.54       102\n",
      "          56       0.00      0.00      0.00         0\n",
      "          57       0.33      0.13      0.19        15\n",
      "          58       0.33      0.19      0.24        16\n",
      "          59       0.50      0.40      0.44         5\n",
      "          60       0.64      0.44      0.52        36\n",
      "          61       0.36      0.22      0.28        18\n",
      "          62       0.75      0.43      0.55         7\n",
      "          63       0.45      0.59      0.51        17\n",
      "\n",
      "    accuracy                           0.59      1999\n",
      "   macro avg       0.45      0.42      0.43      1999\n",
      "weighted avg       0.59      0.59      0.59      1999\n",
      "\n",
      "Base Model F1 Score for DecisionTree: 0.5854529497747178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.00      0.00      0.00        26\n",
      "           2       0.00      0.00      0.00        12\n",
      "           4       1.00      0.06      0.11        36\n",
      "           5       0.71      0.17      0.27        30\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.86      0.58      0.69        31\n",
      "           8       0.50      0.10      0.17        10\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       0.00      0.00      0.00        21\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.86      0.24      0.38        25\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.71      0.60      0.65        90\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.94      0.70      0.80        23\n",
      "          19       0.80      0.79      0.79       178\n",
      "          20       0.75      0.67      0.71         9\n",
      "          21       0.20      0.06      0.09        18\n",
      "          22       0.00      0.00      0.00        14\n",
      "          23       0.00      0.00      0.00        29\n",
      "          24       0.88      0.21      0.33        34\n",
      "          25       0.99      0.78      0.87       107\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.83      0.58      0.68        59\n",
      "          28       0.00      0.00      0.00        13\n",
      "          29       0.00      0.00      0.00         5\n",
      "          30       1.00      0.12      0.22         8\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.84      0.68      0.75       105\n",
      "          33       0.00      0.00      0.00         3\n",
      "          34       0.10      0.89      0.18       123\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       0.00      0.00      0.00        16\n",
      "          37       0.82      0.37      0.51        90\n",
      "          38       0.75      0.08      0.15        37\n",
      "          39       0.82      0.43      0.57        76\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.87      0.32      0.46        82\n",
      "          43       0.00      0.00      0.00        13\n",
      "          44       0.00      0.00      0.00        21\n",
      "          45       0.66      0.32      0.43       180\n",
      "          46       0.00      0.00      0.00        40\n",
      "          47       0.71      0.41      0.52        96\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       1.00      0.33      0.50         3\n",
      "          51       0.78      0.58      0.67        12\n",
      "          52       1.00      0.05      0.09        21\n",
      "          54       1.00      0.06      0.11        18\n",
      "          55       0.70      0.42      0.53       102\n",
      "          57       0.00      0.00      0.00        15\n",
      "          58       0.00      0.00      0.00        16\n",
      "          59       0.00      0.00      0.00         5\n",
      "          60       0.71      0.28      0.40        36\n",
      "          61       0.00      0.00      0.00        18\n",
      "          62       0.00      0.00      0.00         7\n",
      "          63       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.40      1999\n",
      "   macro avg       0.36      0.18      0.21      1999\n",
      "weighted avg       0.61      0.40      0.43      1999\n",
      "\n",
      "Tuned Model F1 Score for DecisionTree: 0.42898615066171847\n",
      "\n",
      "Best model: LogisticRegression with F1 Score: 0.7490151819744962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating models for Cat3\n",
      "Base Model Classification Report for LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       1.00      0.27      0.43        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.50      0.08      0.14        12\n",
      "           9       0.62      0.60      0.61        30\n",
      "          10       1.00      0.67      0.80         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       1.00      0.14      0.25         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       1.00      0.80      0.89         5\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.66      0.64      0.65        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         5\n",
      "          44       0.53      0.84      0.65        31\n",
      "          45       0.65      0.54      0.59        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         6\n",
      "          49       0.55      0.91      0.68        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       1.00      0.50      0.67         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.82      0.75      0.78        12\n",
      "          57       0.00      0.00      0.00         1\n",
      "          61       0.00      0.00      0.00         2\n",
      "          62       0.00      0.00      0.00         2\n",
      "          65       0.00      0.00      0.00        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.86      0.60      0.71        10\n",
      "          69       0.67      1.00      0.80         2\n",
      "          71       1.00      0.50      0.67         4\n",
      "          72       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.00      0.00      0.00         2\n",
      "          88       0.93      0.62      0.74        21\n",
      "          89       0.75      0.50      0.60        18\n",
      "          92       0.00      0.00      0.00         3\n",
      "          93       1.00      0.50      0.67         2\n",
      "          95       0.85      0.92      0.88        24\n",
      "          97       0.38      0.43      0.40         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.36      0.62      0.45        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       0.00      0.00      0.00         4\n",
      "         103       0.00      0.00      0.00        12\n",
      "         105       0.50      0.11      0.18         9\n",
      "         106       1.00      1.00      1.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       0.00      0.00      0.00         3\n",
      "         119       0.95      1.00      0.97        18\n",
      "         120       0.33      0.50      0.40         2\n",
      "         123       1.00      0.33      0.50         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       0.00      0.00      0.00         1\n",
      "         142       0.00      0.00      0.00         5\n",
      "         143       0.91      0.59      0.71        17\n",
      "         144       0.49      0.84      0.62        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       0.73      0.50      0.59        16\n",
      "         148       0.00      0.00      0.00         5\n",
      "         149       1.00      0.29      0.45        17\n",
      "         150       0.50      0.10      0.17        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       0.00      0.00      0.00         1\n",
      "         156       0.70      0.44      0.54        16\n",
      "         158       1.00      0.36      0.53        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.00      0.00      0.00         9\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.56      0.71         9\n",
      "         172       0.00      0.00      0.00         4\n",
      "         173       0.82      0.41      0.55        22\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       0.88      1.00      0.93        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       0.00      0.00      0.00         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       1.00      0.25      0.40        16\n",
      "         186       0.00      0.00      0.00         4\n",
      "         188       0.88      0.58      0.70        12\n",
      "         189       1.00      0.79      0.88        19\n",
      "         190       0.54      0.73      0.62        30\n",
      "         191       1.00      0.25      0.40         8\n",
      "         192       1.00      1.00      1.00         2\n",
      "         193       0.00      0.00      0.00         3\n",
      "         195       0.77      0.96      0.86        28\n",
      "         196       0.55      0.58      0.56        19\n",
      "         197       0.67      0.80      0.73         5\n",
      "         200       1.00      0.17      0.29        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       1.00      1.00      1.00         1\n",
      "         205       1.00      1.00      1.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.50      0.67         6\n",
      "         214       1.00      0.67      0.80         6\n",
      "         215       1.00      0.90      0.95        10\n",
      "         216       0.89      0.89      0.89        27\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         1\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       0.00      0.00      0.00         1\n",
      "         222       0.89      0.50      0.64        16\n",
      "         224       1.00      0.33      0.50         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.84      0.84      0.84        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.67      0.80         6\n",
      "         231       1.00      0.40      0.57        10\n",
      "         232       0.00      0.00      0.00         8\n",
      "         234       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.91      1.00      0.95        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.67      0.73      0.70        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.85      0.74      0.79        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.41      0.67      0.51        21\n",
      "         252       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       1.00      0.33      0.50         3\n",
      "         258       0.00      0.00      0.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.67      0.50      0.57         8\n",
      "         262       1.00      0.40      0.57         5\n",
      "         263       0.00      0.00      0.00         3\n",
      "         264       0.00      0.00      0.00         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      0.50      0.67         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.64      0.78      0.70        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       0.78      0.82      0.80        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.80      0.80      0.80         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       1.00      1.00      1.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       1.00      0.35      0.52        17\n",
      "         296       1.00      0.43      0.60         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       0.00      0.00      0.00         3\n",
      "         307       0.80      0.67      0.73        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.58      0.88      0.70        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       0.00      0.00      0.00         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       1.00      0.14      0.25         7\n",
      "         318       1.00      0.50      0.67         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       0.00      0.00      0.00         8\n",
      "         322       1.00      0.12      0.22         8\n",
      "         323       0.00      0.00      0.00         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       0.00      0.00      0.00         3\n",
      "         334       0.67      0.44      0.53         9\n",
      "         335       0.72      0.86      0.78        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       0.00      0.00      0.00         3\n",
      "         341       1.00      0.33      0.50         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       0.74      0.85      0.79        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       0.00      0.00      0.00         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.54      0.97      0.70        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       1.00      0.40      0.57        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.80      1.00      0.89         4\n",
      "         364       0.84      0.76      0.80        21\n",
      "         365       0.12      0.46      0.20       119\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.51      0.90      0.65       133\n",
      "         369       0.00      0.00      0.00         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         6\n",
      "         374       0.00      0.00      0.00         2\n",
      "         375       0.68      0.96      0.79        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.54      1999\n",
      "   macro avg       0.27      0.20      0.21      1999\n",
      "weighted avg       0.54      0.54      0.50      1999\n",
      "\n",
      "Base Model F1 Score for LogisticRegression: 0.5015559382687471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.67      0.36      0.47        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.67      0.40      0.50         5\n",
      "           8       0.41      0.58      0.48        12\n",
      "           9       0.65      0.57      0.61        30\n",
      "          10       1.00      0.67      0.80         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.50      0.33      0.40         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.20      1.00      0.33         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       1.00      0.86      0.92         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       0.83      1.00      0.91         5\n",
      "          39       0.50      0.50      0.50         2\n",
      "          40       0.87      0.72      0.79        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.40      0.57         5\n",
      "          44       0.59      0.87      0.70        31\n",
      "          45       0.56      0.62      0.59        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.75      0.50      0.60         6\n",
      "          49       0.64      0.94      0.76        32\n",
      "          50       1.00      0.33      0.50         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       1.00      1.00      1.00         1\n",
      "          53       0.67      0.50      0.57         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.79      0.92      0.85        12\n",
      "          57       1.00      1.00      1.00         1\n",
      "          61       1.00      1.00      1.00         2\n",
      "          62       1.00      1.00      1.00         2\n",
      "          65       0.33      0.09      0.14        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.78      0.70      0.74        10\n",
      "          69       0.40      1.00      0.57         2\n",
      "          71       1.00      0.50      0.67         4\n",
      "          72       1.00      1.00      1.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.50      1.00      0.67         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.50      0.50      0.50         2\n",
      "          88       0.71      0.71      0.71        21\n",
      "          89       0.62      0.56      0.59        18\n",
      "          92       1.00      0.67      0.80         3\n",
      "          93       0.67      1.00      0.80         2\n",
      "          95       0.86      1.00      0.92        24\n",
      "          97       0.67      0.86      0.75         7\n",
      "          98       1.00      0.25      0.40         4\n",
      "          99       0.50      0.67      0.57        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       1.00      0.25      0.40         4\n",
      "         103       0.50      0.33      0.40        12\n",
      "         105       0.50      0.56      0.53         9\n",
      "         106       1.00      1.00      1.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         109       0.00      0.00      0.00         0\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       1.00      0.67      0.80         3\n",
      "         119       0.95      1.00      0.97        18\n",
      "         120       0.33      0.50      0.40         2\n",
      "         123       1.00      0.50      0.67         6\n",
      "         124       1.00      0.25      0.40         4\n",
      "         125       0.50      0.50      0.50         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       0.50      1.00      0.67         1\n",
      "         142       1.00      0.20      0.33         5\n",
      "         143       0.70      0.82      0.76        17\n",
      "         144       0.62      0.73      0.67        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       1.00      0.50      0.67         2\n",
      "         147       0.71      0.62      0.67        16\n",
      "         148       1.00      0.60      0.75         5\n",
      "         149       0.73      0.47      0.57        17\n",
      "         150       0.21      0.30      0.25        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       1.00      1.00      1.00         1\n",
      "         156       0.72      0.81      0.76        16\n",
      "         158       0.91      0.91      0.91        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.75      0.67      0.71         9\n",
      "         167       0.50      0.20      0.29         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.78      0.88         9\n",
      "         172       1.00      0.25      0.40         4\n",
      "         173       0.76      0.73      0.74        22\n",
      "         174       1.00      0.50      0.67         2\n",
      "         175       1.00      1.00      1.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       0.93      0.93      0.93        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       1.00      0.80      0.89         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       0.80      0.50      0.62        16\n",
      "         186       0.20      0.25      0.22         4\n",
      "         188       0.90      0.75      0.82        12\n",
      "         189       0.94      0.79      0.86        19\n",
      "         190       0.74      0.67      0.70        30\n",
      "         191       0.43      0.38      0.40         8\n",
      "         192       0.67      1.00      0.80         2\n",
      "         193       1.00      0.67      0.80         3\n",
      "         194       0.00      0.00      0.00         0\n",
      "         195       0.82      1.00      0.90        28\n",
      "         196       0.44      0.58      0.50        19\n",
      "         197       0.67      0.80      0.73         5\n",
      "         200       0.73      0.67      0.70        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.50      1.00      0.67         1\n",
      "         205       1.00      1.00      1.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       1.00      1.00      1.00         1\n",
      "         210       1.00      0.17      0.29         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.83      0.91         6\n",
      "         214       1.00      0.67      0.80         6\n",
      "         215       0.91      1.00      0.95        10\n",
      "         216       0.80      0.89      0.84        27\n",
      "         217       1.00      1.00      1.00         1\n",
      "         218       1.00      1.00      1.00         1\n",
      "         219       1.00      0.33      0.50         3\n",
      "         220       0.00      0.00      0.00         1\n",
      "         222       0.82      0.56      0.67        16\n",
      "         224       1.00      0.67      0.80         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.85      0.87      0.86        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.83      0.91         6\n",
      "         231       0.82      0.90      0.86        10\n",
      "         232       0.20      0.12      0.15         8\n",
      "         234       1.00      1.00      1.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.91      1.00      0.95        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       1.00      1.00      1.00         1\n",
      "         244       0.71      0.83      0.77        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.80      0.87      0.83        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.35      0.76      0.48        21\n",
      "         252       0.60      0.60      0.60         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       0.50      0.33      0.40         3\n",
      "         258       1.00      1.00      1.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       1.00      1.00      1.00         1\n",
      "         261       0.75      0.75      0.75         8\n",
      "         262       0.50      0.40      0.44         5\n",
      "         263       0.50      0.33      0.40         3\n",
      "         264       1.00      0.17      0.29         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      0.50      0.67         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.64      0.78      0.70        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       0.74      0.82      0.78        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       1.00      0.50      0.67         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       1.00      0.33      0.50         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.67      0.80      0.73         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.50      1.00      0.67         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.82      0.53      0.64        17\n",
      "         296       1.00      0.57      0.73         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       1.00      1.00      1.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       1.00      0.67      0.80         3\n",
      "         307       0.80      0.67      0.73        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.68      0.88      0.77        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       1.00      0.67      0.80         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.62      0.71      0.67         7\n",
      "         318       1.00      0.50      0.67         2\n",
      "         319       1.00      0.25      0.40         4\n",
      "         321       0.67      0.25      0.36         8\n",
      "         322       1.00      0.75      0.86         8\n",
      "         323       1.00      0.50      0.67         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       1.00      0.20      0.33         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       1.00      0.67      0.80         3\n",
      "         334       0.55      0.67      0.60         9\n",
      "         335       0.83      0.88      0.85        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       0.67      0.67      0.67         3\n",
      "         341       1.00      1.00      1.00         3\n",
      "         342       1.00      1.00      1.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.50      0.25      0.33         4\n",
      "         345       0.74      1.00      0.85        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       1.00      0.50      0.67         2\n",
      "         349       0.50      0.33      0.40         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.70      0.97      0.81        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       1.00      0.60      0.75        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.80      1.00      0.89         4\n",
      "         364       0.83      0.90      0.86        21\n",
      "         365       0.24      0.38      0.30       119\n",
      "         366       1.00      0.50      0.67         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.59      0.89      0.71       133\n",
      "         369       1.00      1.00      1.00         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         6\n",
      "         374       1.00      0.50      0.67         2\n",
      "         375       0.70      0.98      0.82        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.66      1999\n",
      "   macro avg       0.46      0.40      0.41      1999\n",
      "weighted avg       0.64      0.66      0.63      1999\n",
      "\n",
      "Tuned Model F1 Score for LogisticRegression: 0.6254729432155549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.00      0.00      0.00        12\n",
      "           9       0.86      0.20      0.32        30\n",
      "          10       1.00      0.11      0.20         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       0.00      0.00      0.00         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       0.00      0.00      0.00         5\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.89      0.44      0.59        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         5\n",
      "          44       0.50      0.03      0.06        31\n",
      "          45       0.00      0.00      0.00        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         6\n",
      "          49       0.92      0.75      0.83        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.00      0.00      0.00         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00        12\n",
      "          57       0.00      0.00      0.00         1\n",
      "          61       0.00      0.00      0.00         2\n",
      "          62       0.00      0.00      0.00         2\n",
      "          65       0.00      0.00      0.00        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.00      0.00      0.00        10\n",
      "          69       0.00      0.00      0.00         2\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.00      0.00      0.00         2\n",
      "          88       1.00      0.05      0.09        21\n",
      "          89       1.00      0.17      0.29        18\n",
      "          92       0.00      0.00      0.00         3\n",
      "          93       0.00      0.00      0.00         2\n",
      "          95       1.00      0.54      0.70        24\n",
      "          97       0.00      0.00      0.00         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.86      0.25      0.39        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       0.00      0.00      0.00         4\n",
      "         103       0.00      0.00      0.00        12\n",
      "         105       0.00      0.00      0.00         9\n",
      "         106       0.00      0.00      0.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       0.00      0.00      0.00         3\n",
      "         119       0.95      1.00      0.97        18\n",
      "         120       0.00      0.00      0.00         2\n",
      "         123       0.00      0.00      0.00         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       0.00      0.00      0.00         1\n",
      "         142       0.00      0.00      0.00         5\n",
      "         143       0.00      0.00      0.00        17\n",
      "         144       0.51      0.69      0.59        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       1.00      0.06      0.12        16\n",
      "         148       0.00      0.00      0.00         5\n",
      "         149       0.00      0.00      0.00        17\n",
      "         150       0.00      0.00      0.00        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       0.00      0.00      0.00         1\n",
      "         156       0.00      0.00      0.00        16\n",
      "         158       0.00      0.00      0.00        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.00      0.00      0.00         9\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         9\n",
      "         172       0.00      0.00      0.00         4\n",
      "         173       0.00      0.00      0.00        22\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       1.00      0.29      0.44        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       0.00      0.00      0.00         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       1.00      0.06      0.12        16\n",
      "         186       0.00      0.00      0.00         4\n",
      "         188       0.00      0.00      0.00        12\n",
      "         189       1.00      0.32      0.48        19\n",
      "         190       0.79      0.37      0.50        30\n",
      "         191       0.00      0.00      0.00         8\n",
      "         192       0.00      0.00      0.00         2\n",
      "         193       0.00      0.00      0.00         3\n",
      "         195       0.95      0.64      0.77        28\n",
      "         196       1.00      0.05      0.10        19\n",
      "         197       1.00      0.20      0.33         5\n",
      "         200       0.00      0.00      0.00        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       0.00      0.00      0.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         6\n",
      "         214       1.00      0.67      0.80         6\n",
      "         215       0.00      0.00      0.00        10\n",
      "         216       1.00      0.56      0.71        27\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         1\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       0.00      0.00      0.00         1\n",
      "         222       0.00      0.00      0.00        16\n",
      "         224       0.00      0.00      0.00         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       1.00      0.31      0.47        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       0.00      0.00      0.00         6\n",
      "         231       0.00      0.00      0.00        10\n",
      "         232       0.00      0.00      0.00         8\n",
      "         234       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.78      1.00      0.88        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.89      0.53      0.67        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.90      0.39      0.55        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       1.00      0.19      0.32        21\n",
      "         252       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       0.00      0.00      0.00         3\n",
      "         258       0.00      0.00      0.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.00      0.00      0.00         8\n",
      "         262       0.00      0.00      0.00         5\n",
      "         263       0.00      0.00      0.00         3\n",
      "         264       0.00      0.00      0.00         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       0.00      0.00      0.00         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.80      0.22      0.35        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       1.00      0.06      0.11        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.00      0.00      0.00         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.00      0.00      0.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.00      0.00      0.00        17\n",
      "         296       0.00      0.00      0.00         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       0.00      0.00      0.00         3\n",
      "         307       0.00      0.00      0.00        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.63      0.90      0.74        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       0.00      0.00      0.00         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.00      0.00      0.00         7\n",
      "         318       0.00      0.00      0.00         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       0.00      0.00      0.00         8\n",
      "         322       0.00      0.00      0.00         8\n",
      "         323       0.00      0.00      0.00         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       0.00      0.00      0.00         3\n",
      "         334       1.00      0.11      0.20         9\n",
      "         335       0.80      0.82      0.81        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       0.00      0.00      0.00         3\n",
      "         341       0.00      0.00      0.00         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       1.00      0.20      0.33        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       0.00      0.00      0.00         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.46      0.97      0.63        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       0.00      0.00      0.00        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.00      0.00      0.00         4\n",
      "         364       1.00      0.52      0.69        21\n",
      "         365       0.11      0.57      0.19       119\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.18      0.98      0.30       133\n",
      "         369       0.00      0.00      0.00         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         6\n",
      "         374       0.00      0.00      0.00         2\n",
      "         375       0.48      1.00      0.65        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.33      1999\n",
      "   macro avg       0.12      0.06      0.06      1999\n",
      "weighted avg       0.39      0.33      0.27      1999\n",
      "\n",
      "Base Model F1 Score for MultinomialNB: 0.27471119233559316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.00      0.00      0.00        12\n",
      "           9       0.73      0.37      0.49        30\n",
      "          10       1.00      0.56      0.71         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       0.00      0.00      0.00         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       0.00      0.00      0.00         5\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.83      0.56      0.67        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         5\n",
      "          44       0.67      0.13      0.22        31\n",
      "          45       1.00      0.25      0.40        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         6\n",
      "          49       0.73      0.84      0.78        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.00      0.00      0.00         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       1.00      0.17      0.29        12\n",
      "          57       0.00      0.00      0.00         1\n",
      "          61       0.00      0.00      0.00         2\n",
      "          62       0.00      0.00      0.00         2\n",
      "          65       0.00      0.00      0.00        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.00      0.00      0.00        10\n",
      "          69       0.00      0.00      0.00         2\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.00      0.00      0.00         2\n",
      "          88       1.00      0.24      0.38        21\n",
      "          89       1.00      0.28      0.43        18\n",
      "          92       0.00      0.00      0.00         3\n",
      "          93       0.00      0.00      0.00         2\n",
      "          95       0.95      0.83      0.89        24\n",
      "          97       0.00      0.00      0.00         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.60      0.50      0.55        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       0.00      0.00      0.00         4\n",
      "         103       0.00      0.00      0.00        12\n",
      "         105       0.00      0.00      0.00         9\n",
      "         106       0.00      0.00      0.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       0.00      0.00      0.00         3\n",
      "         119       0.95      1.00      0.97        18\n",
      "         120       0.00      0.00      0.00         2\n",
      "         123       0.00      0.00      0.00         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       0.00      0.00      0.00         1\n",
      "         142       0.00      0.00      0.00         5\n",
      "         143       1.00      0.18      0.30        17\n",
      "         144       0.49      0.82      0.61        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       1.00      0.25      0.40        16\n",
      "         148       0.00      0.00      0.00         5\n",
      "         149       1.00      0.12      0.21        17\n",
      "         150       0.00      0.00      0.00        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       0.00      0.00      0.00         1\n",
      "         156       0.00      0.00      0.00        16\n",
      "         158       0.00      0.00      0.00        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.00      0.00      0.00         9\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.33      0.50         9\n",
      "         172       0.00      0.00      0.00         4\n",
      "         173       1.00      0.14      0.24        22\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       1.00      0.50      0.67        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       0.00      0.00      0.00         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       1.00      0.12      0.22        16\n",
      "         186       0.00      0.00      0.00         4\n",
      "         188       0.00      0.00      0.00        12\n",
      "         189       1.00      0.53      0.69        19\n",
      "         190       0.62      0.53      0.57        30\n",
      "         191       0.00      0.00      0.00         8\n",
      "         192       0.00      0.00      0.00         2\n",
      "         193       0.00      0.00      0.00         3\n",
      "         195       0.96      0.89      0.93        28\n",
      "         196       1.00      0.26      0.42        19\n",
      "         197       1.00      0.80      0.89         5\n",
      "         200       0.00      0.00      0.00        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       1.00      1.00      1.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         6\n",
      "         214       1.00      0.67      0.80         6\n",
      "         215       1.00      0.20      0.33        10\n",
      "         216       0.95      0.74      0.83        27\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         1\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       0.00      0.00      0.00         1\n",
      "         222       1.00      0.06      0.12        16\n",
      "         224       0.00      0.00      0.00         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       1.00      0.40      0.57        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.17      0.29         6\n",
      "         231       0.00      0.00      0.00        10\n",
      "         232       0.00      0.00      0.00         8\n",
      "         234       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.76      1.00      0.87        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.75      0.70      0.72        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.92      0.48      0.63        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.88      0.33      0.48        21\n",
      "         252       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       0.00      0.00      0.00         3\n",
      "         258       0.00      0.00      0.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.00      0.00      0.00         8\n",
      "         262       0.00      0.00      0.00         5\n",
      "         263       0.00      0.00      0.00         3\n",
      "         264       0.00      0.00      0.00         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       0.00      0.00      0.00         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.71      0.28      0.40        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       1.00      0.29      0.45        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.00      0.00      0.00         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.00      0.00      0.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       1.00      0.06      0.11        17\n",
      "         296       1.00      0.14      0.25         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       0.00      0.00      0.00         3\n",
      "         307       0.00      0.00      0.00        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.55      0.92      0.69        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       0.00      0.00      0.00         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.00      0.00      0.00         7\n",
      "         318       0.00      0.00      0.00         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       0.00      0.00      0.00         8\n",
      "         322       0.00      0.00      0.00         8\n",
      "         323       0.00      0.00      0.00         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       0.00      0.00      0.00         3\n",
      "         334       1.00      0.11      0.20         9\n",
      "         335       0.74      0.86      0.80        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       0.00      0.00      0.00         3\n",
      "         341       0.00      0.00      0.00         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       0.80      0.40      0.53        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       0.00      0.00      0.00         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.48      0.97      0.64        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       0.00      0.00      0.00        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.00      0.00      0.00         4\n",
      "         364       1.00      0.57      0.73        21\n",
      "         365       0.11      0.57      0.18       119\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.25      0.96      0.40       133\n",
      "         369       0.00      0.00      0.00         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         6\n",
      "         374       0.00      0.00      0.00         2\n",
      "         375       0.48      1.00      0.65        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.40      1999\n",
      "   macro avg       0.16      0.09      0.10      1999\n",
      "weighted avg       0.46      0.40      0.35      1999\n",
      "\n",
      "Tuned Model F1 Score for MultinomialNB: 0.3455963192497496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      0.50      0.67         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       1.00      0.18      0.31        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.00      0.00      0.00        12\n",
      "           9       0.72      0.60      0.65        30\n",
      "          10       1.00      0.67      0.80         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       0.00      0.00      0.00         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       1.00      0.80      0.89         5\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.87      0.72      0.79        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         5\n",
      "          44       0.61      0.81      0.69        31\n",
      "          45       0.63      0.50      0.56        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         6\n",
      "          49       0.69      0.91      0.78        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       1.00      0.50      0.67         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.91      0.83      0.87        12\n",
      "          57       0.00      0.00      0.00         1\n",
      "          61       1.00      0.50      0.67         2\n",
      "          62       0.00      0.00      0.00         2\n",
      "          65       0.00      0.00      0.00        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       1.00      0.60      0.75        10\n",
      "          69       1.00      1.00      1.00         2\n",
      "          71       1.00      0.50      0.67         4\n",
      "          72       1.00      1.00      1.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.67      0.50      0.57         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.00      0.00      0.00         2\n",
      "          88       1.00      0.38      0.55        21\n",
      "          89       0.80      0.44      0.57        18\n",
      "          92       0.00      0.00      0.00         3\n",
      "          93       1.00      0.50      0.67         2\n",
      "          95       0.91      0.88      0.89        24\n",
      "          97       0.38      0.43      0.40         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.48      0.58      0.53        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       0.00      0.00      0.00         4\n",
      "         103       0.00      0.00      0.00        12\n",
      "         105       1.00      0.22      0.36         9\n",
      "         106       1.00      1.00      1.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       1.00      0.33      0.50         3\n",
      "         119       0.95      1.00      0.97        18\n",
      "         120       0.33      0.50      0.40         2\n",
      "         123       1.00      0.33      0.50         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       0.00      0.00      0.00         1\n",
      "         142       1.00      0.20      0.33         5\n",
      "         143       0.85      0.65      0.73        17\n",
      "         144       0.55      0.76      0.64        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       0.88      0.44      0.58        16\n",
      "         148       1.00      0.60      0.75         5\n",
      "         149       1.00      0.29      0.45        17\n",
      "         150       0.00      0.00      0.00        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       0.00      0.00      0.00         1\n",
      "         156       0.73      0.50      0.59        16\n",
      "         158       1.00      0.36      0.53        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       1.00      0.22      0.36         9\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.56      0.71         9\n",
      "         172       1.00      0.25      0.40         4\n",
      "         173       0.90      0.41      0.56        22\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       1.00      0.86      0.92        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       1.00      0.40      0.57         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       1.00      0.31      0.48        16\n",
      "         186       0.00      0.00      0.00         4\n",
      "         188       0.88      0.58      0.70        12\n",
      "         189       1.00      0.74      0.85        19\n",
      "         190       0.61      0.67      0.63        30\n",
      "         191       0.00      0.00      0.00         8\n",
      "         192       1.00      1.00      1.00         2\n",
      "         193       0.00      0.00      0.00         3\n",
      "         195       0.87      0.96      0.92        28\n",
      "         196       0.75      0.47      0.58        19\n",
      "         197       1.00      0.80      0.89         5\n",
      "         200       1.00      0.25      0.40        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       1.00      1.00      1.00         1\n",
      "         205       1.00      1.00      1.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.67      0.80         6\n",
      "         214       1.00      0.67      0.80         6\n",
      "         215       1.00      0.90      0.95        10\n",
      "         216       0.95      0.78      0.86        27\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         1\n",
      "         219       1.00      0.33      0.50         3\n",
      "         220       0.00      0.00      0.00         1\n",
      "         222       0.89      0.50      0.64        16\n",
      "         224       1.00      0.33      0.50         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.87      0.73      0.80        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.67      0.80         6\n",
      "         231       1.00      0.40      0.57        10\n",
      "         232       0.00      0.00      0.00         8\n",
      "         234       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.93      1.00      0.96        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.86      0.80      0.83        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.84      0.70      0.76        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.67      0.67      0.67        21\n",
      "         252       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       1.00      0.33      0.50         3\n",
      "         258       0.00      0.00      0.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.71      0.62      0.67         8\n",
      "         262       1.00      0.40      0.57         5\n",
      "         263       0.00      0.00      0.00         3\n",
      "         264       0.00      0.00      0.00         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      0.50      0.67         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.61      0.61      0.61        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       0.80      0.71      0.75        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.67      0.80      0.73         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       1.00      1.00      1.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       1.00      0.29      0.45        17\n",
      "         296       1.00      0.43      0.60         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       0.00      0.00      0.00         3\n",
      "         307       0.80      0.67      0.73        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.69      0.86      0.77        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       0.00      0.00      0.00         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       1.00      0.29      0.44         7\n",
      "         318       1.00      0.50      0.67         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       0.00      0.00      0.00         8\n",
      "         322       1.00      0.75      0.86         8\n",
      "         323       1.00      0.50      0.67         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       1.00      0.33      0.50         3\n",
      "         334       0.50      0.56      0.53         9\n",
      "         335       0.80      0.86      0.83        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       0.00      0.00      0.00         3\n",
      "         341       1.00      0.67      0.80         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       0.82      0.90      0.86        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       0.00      0.00      0.00         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.78      0.90      0.84        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       1.00      0.40      0.57        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.80      1.00      0.89         4\n",
      "         364       0.89      0.81      0.85        21\n",
      "         365       0.11      0.57      0.18       119\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.58      0.89      0.70       133\n",
      "         369       1.00      0.67      0.80         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         6\n",
      "         374       1.00      0.50      0.67         2\n",
      "         375       0.73      0.96      0.83        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.55      1999\n",
      "   macro avg       0.33      0.23      0.25      1999\n",
      "weighted avg       0.60      0.55      0.53      1999\n",
      "\n",
      "Base Model F1 Score for SVC: 0.5347298741431233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.67      0.36      0.47        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.67      0.40      0.50         5\n",
      "           8       0.44      0.67      0.53        12\n",
      "           9       0.64      0.53      0.58        30\n",
      "          10       1.00      0.67      0.80         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.57      0.67      0.62         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "          22       0.25      1.00      0.40         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          24       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       1.00      0.71      0.83         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       0.71      1.00      0.83         5\n",
      "          39       0.50      0.50      0.50         2\n",
      "          40       0.82      0.75      0.78        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.40      0.57         5\n",
      "          44       0.60      0.81      0.68        31\n",
      "          45       0.57      0.71      0.63        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.75      0.50      0.60         6\n",
      "          49       0.71      0.94      0.81        32\n",
      "          50       1.00      0.33      0.50         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       1.00      1.00      1.00         1\n",
      "          53       0.83      0.62      0.71         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.83      0.83      0.83        12\n",
      "          57       0.00      0.00      0.00         1\n",
      "          61       1.00      1.00      1.00         2\n",
      "          62       1.00      1.00      1.00         2\n",
      "          65       0.43      0.27      0.33        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.88      0.70      0.78        10\n",
      "          69       0.50      1.00      0.67         2\n",
      "          71       1.00      0.75      0.86         4\n",
      "          72       1.00      1.00      1.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.80      1.00      0.89         4\n",
      "          83       1.00      1.00      1.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       1.00      0.50      0.67         2\n",
      "          88       0.78      0.67      0.72        21\n",
      "          89       0.67      0.56      0.61        18\n",
      "          92       1.00      0.67      0.80         3\n",
      "          93       1.00      1.00      1.00         2\n",
      "          95       0.88      0.92      0.90        24\n",
      "          96       0.00      0.00      0.00         0\n",
      "          97       0.62      0.71      0.67         7\n",
      "          98       0.50      0.25      0.33         4\n",
      "          99       0.54      0.62      0.58        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       1.00      0.25      0.40         4\n",
      "         103       0.67      0.33      0.44        12\n",
      "         105       0.46      0.67      0.55         9\n",
      "         106       1.00      1.00      1.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         109       0.00      0.00      0.00         0\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       1.00      0.67      0.80         3\n",
      "         119       0.95      1.00      0.97        18\n",
      "         120       0.50      0.50      0.50         2\n",
      "         123       1.00      0.67      0.80         6\n",
      "         124       0.50      0.25      0.33         4\n",
      "         125       0.60      0.75      0.67         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.50      1.00      0.67         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       0.50      1.00      0.67         1\n",
      "         137       0.50      1.00      0.67         1\n",
      "         142       1.00      0.40      0.57         5\n",
      "         143       0.61      0.82      0.70        17\n",
      "         144       0.65      0.73      0.68        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       1.00      0.50      0.67         2\n",
      "         147       0.69      0.69      0.69        16\n",
      "         148       1.00      0.60      0.75         5\n",
      "         149       0.70      0.41      0.52        17\n",
      "         150       0.13      0.20      0.16        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       1.00      1.00      1.00         1\n",
      "         156       0.79      0.69      0.73        16\n",
      "         158       0.90      0.82      0.86        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.86      0.67      0.75         9\n",
      "         167       0.75      0.60      0.67         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.78      0.88         9\n",
      "         172       1.00      0.25      0.40         4\n",
      "         173       0.76      0.73      0.74        22\n",
      "         174       1.00      0.50      0.67         2\n",
      "         175       1.00      1.00      1.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       1.00      0.93      0.96        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       1.00      0.80      0.89         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       0.73      0.50      0.59        16\n",
      "         186       0.25      0.25      0.25         4\n",
      "         188       1.00      0.75      0.86        12\n",
      "         189       0.94      0.79      0.86        19\n",
      "         190       0.70      0.70      0.70        30\n",
      "         191       0.38      0.38      0.38         8\n",
      "         192       0.67      1.00      0.80         2\n",
      "         193       1.00      0.67      0.80         3\n",
      "         194       0.00      0.00      0.00         0\n",
      "         195       0.82      0.96      0.89        28\n",
      "         196       0.39      0.47      0.43        19\n",
      "         197       0.67      0.80      0.73         5\n",
      "         200       0.80      0.67      0.73        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.50      1.00      0.67         1\n",
      "         205       1.00      1.00      1.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       1.00      1.00      1.00         1\n",
      "         210       0.67      0.33      0.44         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.83      0.91         6\n",
      "         214       1.00      0.67      0.80         6\n",
      "         215       1.00      1.00      1.00        10\n",
      "         216       0.80      0.89      0.84        27\n",
      "         217       1.00      1.00      1.00         1\n",
      "         218       1.00      1.00      1.00         1\n",
      "         219       1.00      0.33      0.50         3\n",
      "         220       1.00      1.00      1.00         1\n",
      "         222       0.80      0.50      0.62        16\n",
      "         224       1.00      1.00      1.00         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.80      0.82      0.81        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.83      0.91         6\n",
      "         231       0.83      1.00      0.91        10\n",
      "         232       0.33      0.12      0.18         8\n",
      "         234       1.00      1.00      1.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.93      0.97      0.95        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       1.00      1.00      1.00         1\n",
      "         244       0.74      0.83      0.78        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.74      0.87      0.80        23\n",
      "         249       1.00      1.00      1.00         1\n",
      "         251       0.38      0.71      0.50        21\n",
      "         252       0.50      0.40      0.44         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       0.67      0.67      0.67         3\n",
      "         258       1.00      0.50      0.67         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       1.00      1.00      1.00         1\n",
      "         261       0.75      0.75      0.75         8\n",
      "         262       0.50      0.40      0.44         5\n",
      "         263       0.50      0.33      0.40         3\n",
      "         264       1.00      0.17      0.29         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      0.50      0.67         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.64      0.78      0.70        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       0.78      0.82      0.80        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       1.00      0.50      0.67         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       1.00      0.33      0.50         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       1.00      1.00      1.00         1\n",
      "         289       0.67      0.80      0.73         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.67      1.00      0.80         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.62      0.47      0.53        17\n",
      "         296       1.00      0.57      0.73         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       1.00      1.00      1.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       1.00      0.50      0.67         2\n",
      "         305       1.00      0.67      0.80         3\n",
      "         307       0.67      0.67      0.67        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.74      0.82      0.78        51\n",
      "         310       1.00      0.50      0.67         2\n",
      "         312       1.00      0.67      0.80         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.71      0.71      0.71         7\n",
      "         318       1.00      0.50      0.67         2\n",
      "         319       1.00      0.25      0.40         4\n",
      "         321       0.40      0.25      0.31         8\n",
      "         322       0.86      0.75      0.80         8\n",
      "         323       1.00      0.50      0.67         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       1.00      0.20      0.33         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.50      0.25      0.33         4\n",
      "         332       1.00      0.67      0.80         3\n",
      "         334       0.50      0.78      0.61         9\n",
      "         335       0.82      0.84      0.83        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       1.00      0.67      0.80         3\n",
      "         341       1.00      1.00      1.00         3\n",
      "         342       1.00      1.00      1.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.50      0.25      0.33         4\n",
      "         345       0.83      0.95      0.88        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.50      0.50      0.50         2\n",
      "         349       0.50      0.33      0.40         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.50      1.00      0.67         1\n",
      "         359       0.88      0.93      0.90        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       1.00      0.60      0.75        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.80      1.00      0.89         4\n",
      "         364       0.87      0.95      0.91        21\n",
      "         365       0.21      0.42      0.28       119\n",
      "         366       1.00      0.50      0.67         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.64      0.87      0.74       133\n",
      "         369       1.00      1.00      1.00         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       1.00      0.33      0.50         6\n",
      "         374       1.00      0.50      0.67         2\n",
      "         375       0.77      0.89      0.83        46\n",
      "         376       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.66      1999\n",
      "   macro avg       0.49      0.43      0.44      1999\n",
      "weighted avg       0.66      0.66      0.64      1999\n",
      "\n",
      "Tuned Model F1 Score for SVC: 0.6393482935360599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.67      0.36      0.47        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.75      0.60      0.67         5\n",
      "           8       0.15      0.25      0.19        12\n",
      "           9       0.55      0.60      0.57        30\n",
      "          10       1.00      0.78      0.88         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.50      0.50      0.50         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.17      1.00      0.29         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       1.00      0.50      0.67         2\n",
      "          33       1.00      0.86      0.92         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       0.83      1.00      0.91         5\n",
      "          39       1.00      0.50      0.67         2\n",
      "          40       0.78      0.78      0.78        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.40      0.57         5\n",
      "          44       0.46      0.87      0.60        31\n",
      "          45       0.55      0.50      0.52        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       1.00      0.17      0.29         6\n",
      "          49       0.60      0.91      0.72        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       1.00      1.00      1.00         1\n",
      "          53       0.50      0.62      0.56         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.61      0.92      0.73        12\n",
      "          57       0.50      1.00      0.67         1\n",
      "          61       1.00      0.50      0.67         2\n",
      "          62       1.00      1.00      1.00         2\n",
      "          65       0.00      0.00      0.00        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.50      1.00      0.67         1\n",
      "          68       0.78      0.70      0.74        10\n",
      "          69       0.67      1.00      0.80         2\n",
      "          71       0.67      0.50      0.57         4\n",
      "          72       1.00      1.00      1.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.43      0.75      0.55         4\n",
      "          83       1.00      1.00      1.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       1.00      0.50      0.67         2\n",
      "          88       0.60      0.71      0.65        21\n",
      "          89       0.69      0.61      0.65        18\n",
      "          92       1.00      0.33      0.50         3\n",
      "          93       0.50      1.00      0.67         2\n",
      "          95       0.83      0.83      0.83        24\n",
      "          97       0.50      0.57      0.53         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.35      0.62      0.45        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       1.00      0.25      0.40         4\n",
      "         103       0.40      0.17      0.24        12\n",
      "         105       0.25      0.44      0.32         9\n",
      "         106       1.00      1.00      1.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         109       0.00      0.00      0.00         0\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       1.00      0.67      0.80         3\n",
      "         119       0.90      1.00      0.95        18\n",
      "         120       0.25      0.50      0.33         2\n",
      "         123       1.00      0.50      0.67         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.33      0.50      0.40         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       1.00      0.50      0.67         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       1.00      1.00      1.00         1\n",
      "         142       1.00      0.20      0.33         5\n",
      "         143       0.71      0.71      0.71        17\n",
      "         144       0.52      0.84      0.64        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       1.00      0.50      0.67         2\n",
      "         147       0.69      0.56      0.62        16\n",
      "         148       1.00      0.40      0.57         5\n",
      "         149       0.58      0.41      0.48        17\n",
      "         150       0.07      0.20      0.11        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       1.00      1.00      1.00         1\n",
      "         156       0.75      0.56      0.64        16\n",
      "         158       0.71      0.45      0.56        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.46      0.67      0.55         9\n",
      "         167       0.67      0.40      0.50         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.89      0.94         9\n",
      "         172       1.00      0.25      0.40         4\n",
      "         173       0.64      0.41      0.50        22\n",
      "         174       1.00      0.50      0.67         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       0.80      0.86      0.83        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       1.00      0.40      0.57         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       0.88      0.44      0.58        16\n",
      "         186       0.25      0.25      0.25         4\n",
      "         188       0.90      0.75      0.82        12\n",
      "         189       1.00      0.79      0.88        19\n",
      "         190       0.49      0.63      0.55        30\n",
      "         191       0.67      0.25      0.36         8\n",
      "         192       0.67      1.00      0.80         2\n",
      "         193       1.00      0.67      0.80         3\n",
      "         194       0.00      0.00      0.00         0\n",
      "         195       0.80      1.00      0.89        28\n",
      "         196       0.44      0.58      0.50        19\n",
      "         197       0.67      0.80      0.73         5\n",
      "         200       0.54      0.58      0.56        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.20      1.00      0.33         1\n",
      "         205       1.00      1.00      1.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       1.00      1.00      1.00         1\n",
      "         210       0.00      0.00      0.00         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      1.00      1.00         6\n",
      "         214       1.00      0.83      0.91         6\n",
      "         215       0.91      1.00      0.95        10\n",
      "         216       0.87      0.74      0.80        27\n",
      "         217       1.00      1.00      1.00         1\n",
      "         218       1.00      1.00      1.00         1\n",
      "         219       1.00      0.33      0.50         3\n",
      "         220       0.00      0.00      0.00         1\n",
      "         222       0.55      0.69      0.61        16\n",
      "         224       1.00      0.33      0.50         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.82      0.82      0.82        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.83      0.91         6\n",
      "         231       1.00      0.90      0.95        10\n",
      "         232       0.14      0.12      0.13         8\n",
      "         234       1.00      1.00      1.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.93      1.00      0.96        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       1.00      1.00      1.00         1\n",
      "         244       0.71      0.80      0.75        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.88      0.61      0.72        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.23      0.62      0.33        21\n",
      "         252       0.67      0.40      0.50         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       0.67      0.67      0.67         3\n",
      "         258       0.80      1.00      0.89         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       1.00      1.00      1.00         1\n",
      "         261       0.60      0.75      0.67         8\n",
      "         262       0.50      0.40      0.44         5\n",
      "         263       0.50      0.33      0.40         3\n",
      "         264       0.00      0.00      0.00         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      0.50      0.67         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.61      0.78      0.68        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       0.67      0.82      0.74        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       1.00      0.25      0.40         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       1.00      0.33      0.50         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.57      0.80      0.67         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.00      0.00      0.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.48      0.59      0.53        17\n",
      "         296       0.80      0.57      0.67         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       1.00      0.67      0.80         3\n",
      "         307       0.67      0.67      0.67        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.59      0.88      0.71        51\n",
      "         310       1.00      0.50      0.67         2\n",
      "         312       1.00      0.67      0.80         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.44      0.57      0.50         7\n",
      "         318       0.33      0.50      0.40         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       0.00      0.00      0.00         8\n",
      "         322       1.00      0.75      0.86         8\n",
      "         323       0.00      0.00      0.00         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       1.00      0.67      0.80         3\n",
      "         334       0.45      0.56      0.50         9\n",
      "         335       0.76      0.88      0.81        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       1.00      0.33      0.50         3\n",
      "         341       0.75      1.00      0.86         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       1.00      1.00      1.00         1\n",
      "         344       1.00      0.25      0.40         4\n",
      "         345       0.64      0.90      0.75        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       0.50      0.33      0.40         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         354       0.00      0.00      0.00         0\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.69      0.93      0.79        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       0.64      0.47      0.54        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.80      1.00      0.89         4\n",
      "         364       0.69      0.86      0.77        21\n",
      "         365       0.36      0.24      0.29       119\n",
      "         366       1.00      0.50      0.67         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.59      0.88      0.70       133\n",
      "         369       1.00      1.00      1.00         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         6\n",
      "         374       1.00      0.50      0.67         2\n",
      "         375       0.63      0.98      0.77        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.61      1999\n",
      "   macro avg       0.41      0.37      0.37      1999\n",
      "weighted avg       0.58      0.61      0.57      1999\n",
      "\n",
      "Base Model F1 Score for RandomForest: 0.5726980474764394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.57      0.36      0.44        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       1.00      0.17      0.29        12\n",
      "           9       0.47      0.60      0.53        30\n",
      "          10       1.00      0.78      0.88         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       1.00      0.14      0.25         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       1.00      1.00      1.00         5\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.74      0.78      0.76        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.20      0.33         5\n",
      "          44       0.44      0.87      0.59        31\n",
      "          45       0.50      0.29      0.37        24\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         6\n",
      "          49       0.40      0.94      0.56        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.75      0.38      0.50         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.65      0.92      0.76        12\n",
      "          57       0.00      0.00      0.00         1\n",
      "          61       0.00      0.00      0.00         2\n",
      "          62       1.00      1.00      1.00         2\n",
      "          65       0.00      0.00      0.00        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       1.00      0.50      0.67        10\n",
      "          69       0.67      1.00      0.80         2\n",
      "          71       1.00      0.50      0.67         4\n",
      "          72       1.00      1.00      1.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       1.00      0.75      0.86         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.00      0.00      0.00         2\n",
      "          88       0.72      0.62      0.67        21\n",
      "          89       0.75      0.50      0.60        18\n",
      "          92       0.00      0.00      0.00         3\n",
      "          93       0.00      0.00      0.00         2\n",
      "          95       0.79      0.92      0.85        24\n",
      "          97       0.38      0.43      0.40         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.32      0.62      0.42        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       0.00      0.00      0.00         4\n",
      "         103       0.00      0.00      0.00        12\n",
      "         105       1.00      0.33      0.50         9\n",
      "         106       1.00      0.50      0.67         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       1.00      0.67      0.80         3\n",
      "         119       0.86      1.00      0.92        18\n",
      "         120       0.00      0.00      0.00         2\n",
      "         123       0.00      0.00      0.00         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.00      0.00      0.00         1\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       0.00      0.00      0.00         1\n",
      "         142       0.00      0.00      0.00         5\n",
      "         143       0.64      0.53      0.58        17\n",
      "         144       0.43      0.82      0.56        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       1.00      0.50      0.67         2\n",
      "         147       0.90      0.56      0.69        16\n",
      "         148       0.00      0.00      0.00         5\n",
      "         149       0.86      0.35      0.50        17\n",
      "         150       0.40      0.20      0.27        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       0.00      0.00      0.00         1\n",
      "         156       0.71      0.31      0.43        16\n",
      "         158       1.00      0.18      0.31        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.56      0.56      0.56         9\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.44      0.62         9\n",
      "         172       0.00      0.00      0.00         4\n",
      "         173       0.80      0.36      0.50        22\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       0.73      0.79      0.76        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       1.00      0.20      0.33         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       1.00      0.25      0.40        16\n",
      "         186       0.00      0.00      0.00         4\n",
      "         188       0.89      0.67      0.76        12\n",
      "         189       1.00      0.79      0.88        19\n",
      "         190       0.50      0.70      0.58        30\n",
      "         191       0.00      0.00      0.00         8\n",
      "         192       0.50      1.00      0.67         2\n",
      "         193       0.00      0.00      0.00         3\n",
      "         195       0.82      1.00      0.90        28\n",
      "         196       0.37      0.53      0.43        19\n",
      "         197       0.57      0.80      0.67         5\n",
      "         200       1.00      0.17      0.29        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       1.00      1.00      1.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         210       0.00      0.00      0.00         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.83      0.91         6\n",
      "         214       1.00      0.83      0.91         6\n",
      "         215       1.00      0.70      0.82        10\n",
      "         216       0.90      0.70      0.79        27\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         1\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       0.00      0.00      0.00         1\n",
      "         222       0.64      0.44      0.52        16\n",
      "         224       0.00      0.00      0.00         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.83      0.84      0.84        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.83      0.91         6\n",
      "         231       1.00      0.10      0.18        10\n",
      "         232       0.00      0.00      0.00         8\n",
      "         234       1.00      1.00      1.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.72      1.00      0.84        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.79      0.77      0.78        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       1.00      0.57      0.72        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.45      0.48      0.47        21\n",
      "         252       0.00      0.00      0.00         5\n",
      "         254       0.00      0.00      0.00         1\n",
      "         255       0.00      0.00      0.00         3\n",
      "         258       0.00      0.00      0.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       0.67      0.75      0.71         8\n",
      "         262       0.50      0.40      0.44         5\n",
      "         263       0.50      0.33      0.40         3\n",
      "         264       0.00      0.00      0.00         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      1.00      1.00         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.58      0.78      0.67        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       0.78      0.82      0.80        17\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.80      0.80      0.80         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       0.00      0.00      0.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.67      0.59      0.62        17\n",
      "         296       0.80      0.57      0.67         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       0.00      0.00      0.00         3\n",
      "         307       0.64      0.58      0.61        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.51      0.90      0.65        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       1.00      0.67      0.80         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.67      0.29      0.40         7\n",
      "         318       0.00      0.00      0.00         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       1.00      0.12      0.22         8\n",
      "         322       1.00      0.12      0.22         8\n",
      "         323       0.00      0.00      0.00         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       0.00      0.00      0.00         3\n",
      "         334       0.40      0.44      0.42         9\n",
      "         335       0.66      0.86      0.75        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       0.00      0.00      0.00         3\n",
      "         341       0.00      0.00      0.00         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       0.49      0.90      0.63        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.00      0.00      0.00         2\n",
      "         349       0.00      0.00      0.00         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.46      0.97      0.62        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       1.00      0.47      0.64        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       1.00      0.75      0.86         4\n",
      "         364       0.70      0.76      0.73        21\n",
      "         365       0.15      0.35      0.21       119\n",
      "         366       1.00      0.25      0.40         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.51      0.89      0.65       133\n",
      "         369       1.00      0.33      0.50         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.00      0.00      0.00         6\n",
      "         374       1.00      0.50      0.67         2\n",
      "         375       0.60      0.98      0.74        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.54      1999\n",
      "   macro avg       0.27      0.21      0.22      1999\n",
      "weighted avg       0.51      0.54      0.48      1999\n",
      "\n",
      "Tuned Model F1 Score for RandomForest: 0.4837792711334628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Classification Report for DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.67      0.31         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.80      0.36      0.50        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       0.17      0.33      0.23        12\n",
      "           9       0.67      0.47      0.55        30\n",
      "          10       0.83      0.56      0.67         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.43      0.50      0.46         6\n",
      "          15       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.33      0.33      0.33         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.50      0.50      0.50         2\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       1.00      0.86      0.92         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       0.83      1.00      0.91         5\n",
      "          39       0.50      0.50      0.50         2\n",
      "          40       0.84      0.72      0.78        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.75      0.60      0.67         5\n",
      "          44       0.59      0.74      0.66        31\n",
      "          45       0.41      0.50      0.45        24\n",
      "          47       1.00      1.00      1.00         1\n",
      "          48       0.11      0.17      0.13         6\n",
      "          49       0.64      0.72      0.68        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.50      1.00      0.67         1\n",
      "          53       0.67      0.25      0.36         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.50      0.42      0.45        12\n",
      "          57       0.50      1.00      0.67         1\n",
      "          58       0.00      0.00      0.00         0\n",
      "          61       1.00      1.00      1.00         2\n",
      "          62       1.00      1.00      1.00         2\n",
      "          65       0.30      0.27      0.29        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.83      0.50      0.62        10\n",
      "          69       0.67      1.00      0.80         2\n",
      "          71       1.00      0.50      0.67         4\n",
      "          72       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         0\n",
      "          81       0.60      0.75      0.67         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          84       0.00      0.00      0.00         0\n",
      "          85       0.00      0.00      0.00         0\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.50      0.50      0.50         2\n",
      "          88       0.67      0.67      0.67        21\n",
      "          89       0.58      0.39      0.47        18\n",
      "          90       0.00      0.00      0.00         0\n",
      "          91       0.00      0.00      0.00         0\n",
      "          92       1.00      0.33      0.50         3\n",
      "          93       0.25      0.50      0.33         2\n",
      "          95       0.91      0.83      0.87        24\n",
      "          97       0.50      0.29      0.36         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.44      0.62      0.52        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       1.00      0.25      0.40         4\n",
      "         103       0.14      0.08      0.11        12\n",
      "         105       0.27      0.67      0.39         9\n",
      "         106       1.00      1.00      1.00         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         109       0.00      0.00      0.00         0\n",
      "         111       1.00      1.00      1.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.25      0.25      0.25         4\n",
      "         118       0.50      0.67      0.57         3\n",
      "         119       1.00      0.94      0.97        18\n",
      "         120       0.50      0.50      0.50         2\n",
      "         123       1.00      0.67      0.80         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.50      0.50      0.50         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.50      1.00      0.67         1\n",
      "         130       0.50      0.67      0.57         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       0.50      1.00      0.67         1\n",
      "         137       0.00      0.00      0.00         1\n",
      "         139       0.00      0.00      0.00         0\n",
      "         142       1.00      0.40      0.57         5\n",
      "         143       0.65      0.65      0.65        17\n",
      "         144       0.53      0.71      0.60        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       1.00      0.50      0.67         2\n",
      "         147       0.33      0.50      0.40        16\n",
      "         148       0.67      0.40      0.50         5\n",
      "         149       0.40      0.35      0.38        17\n",
      "         150       0.05      0.10      0.06        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       1.00      1.00      1.00         1\n",
      "         156       0.50      0.44      0.47        16\n",
      "         158       0.33      0.18      0.24        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.42      0.56      0.48         9\n",
      "         167       0.40      0.40      0.40         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.67      0.80         9\n",
      "         172       0.50      0.25      0.33         4\n",
      "         173       0.57      0.36      0.44        22\n",
      "         174       0.50      0.50      0.50         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       0.90      0.64      0.75        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       0.50      0.40      0.44         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       0.70      0.44      0.54        16\n",
      "         186       0.33      0.25      0.29         4\n",
      "         188       0.89      0.67      0.76        12\n",
      "         189       1.00      0.79      0.88        19\n",
      "         190       0.57      0.53      0.55        30\n",
      "         191       0.60      0.38      0.46         8\n",
      "         192       0.33      0.50      0.40         2\n",
      "         193       1.00      0.67      0.80         3\n",
      "         194       0.00      0.00      0.00         0\n",
      "         195       0.84      0.96      0.90        28\n",
      "         196       0.35      0.42      0.38        19\n",
      "         197       0.33      0.80      0.47         5\n",
      "         200       0.29      0.42      0.34        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         202       0.00      0.00      0.00         0\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       0.50      1.00      0.67         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       1.00      1.00      1.00         1\n",
      "         210       1.00      0.17      0.29         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.67      0.80         6\n",
      "         214       1.00      0.83      0.91         6\n",
      "         215       0.90      0.90      0.90        10\n",
      "         216       0.71      0.81      0.76        27\n",
      "         217       0.50      1.00      0.67         1\n",
      "         218       0.50      1.00      0.67         1\n",
      "         219       1.00      0.33      0.50         3\n",
      "         220       1.00      1.00      1.00         1\n",
      "         221       0.00      0.00      0.00         0\n",
      "         222       0.53      0.56      0.55        16\n",
      "         224       1.00      0.33      0.50         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.76      0.69      0.72        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       1.00      0.83      0.91         6\n",
      "         231       0.70      0.70      0.70        10\n",
      "         232       0.00      0.00      0.00         8\n",
      "         234       1.00      1.00      1.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.88      0.95      0.91        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       1.00      1.00      1.00         1\n",
      "         244       0.63      0.80      0.71        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.77      0.74      0.76        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.30      0.52      0.38        21\n",
      "         252       0.43      0.60      0.50         5\n",
      "         254       1.00      1.00      1.00         1\n",
      "         255       0.50      0.67      0.57         3\n",
      "         258       1.00      0.75      0.86         4\n",
      "         259       1.00      0.50      0.67         4\n",
      "         260       0.50      1.00      0.67         1\n",
      "         261       0.57      0.50      0.53         8\n",
      "         262       0.00      0.00      0.00         5\n",
      "         263       0.00      0.00      0.00         3\n",
      "         264       0.33      0.17      0.22         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       1.00      0.50      0.67         2\n",
      "         272       0.00      0.00      0.00         0\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.61      0.78      0.68        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       0.56      0.59      0.57        17\n",
      "         280       1.00      1.00      1.00         2\n",
      "         281       1.00      0.50      0.67         4\n",
      "         282       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       1.00      0.33      0.50         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.67      0.80      0.73         5\n",
      "         290       0.00      0.00      0.00         0\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       1.00      1.00      1.00         2\n",
      "         293       1.00      1.00      1.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.50      0.47      0.48        17\n",
      "         296       1.00      0.57      0.73         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       1.00      1.00      1.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       1.00      0.67      0.80         3\n",
      "         307       0.36      0.33      0.35        12\n",
      "         308       0.25      0.25      0.25         4\n",
      "         309       0.63      0.76      0.69        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       0.67      0.67      0.67         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.50      0.57      0.53         7\n",
      "         318       0.00      0.00      0.00         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       0.11      0.12      0.12         8\n",
      "         322       0.60      0.38      0.46         8\n",
      "         323       0.50      0.50      0.50         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       1.00      0.40      0.57         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       1.00      0.67      0.80         3\n",
      "         334       0.12      0.22      0.16         9\n",
      "         335       0.78      0.72      0.75        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       1.00      0.33      0.50         3\n",
      "         341       0.25      0.67      0.36         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       1.00      1.00      1.00         1\n",
      "         344       1.00      0.25      0.40         4\n",
      "         345       0.68      0.85      0.76        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.50      0.50      0.50         2\n",
      "         349       0.50      0.33      0.40         3\n",
      "         352       0.20      1.00      0.33         1\n",
      "         354       0.00      0.00      0.00         0\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         358       0.00      0.00      0.00         0\n",
      "         359       0.72      0.90      0.80        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       0.50      0.60      0.55        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.67      1.00      0.80         4\n",
      "         364       0.65      0.62      0.63        21\n",
      "         365       0.32      0.22      0.26       119\n",
      "         366       0.25      0.25      0.25         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.58      0.78      0.67       133\n",
      "         369       0.50      0.33      0.40         3\n",
      "         371       0.33      0.25      0.29         4\n",
      "         372       0.33      0.17      0.22         6\n",
      "         374       1.00      0.50      0.67         2\n",
      "         375       0.66      0.85      0.74        46\n",
      "         376       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.55      1999\n",
      "   macro avg       0.37      0.35      0.34      1999\n",
      "weighted avg       0.56      0.55      0.54      1999\n",
      "\n",
      "Base Model F1 Score for DecisionTree: 0.5400739609500711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Classification Report for DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.80      0.36      0.50        11\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       1.00      0.40      0.57         5\n",
      "           8       0.20      0.08      0.12        12\n",
      "           9       0.70      0.47      0.56        30\n",
      "          10       0.83      0.56      0.67         9\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         6\n",
      "          17       0.00      0.00      0.00         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         2\n",
      "          26       0.00      0.00      0.00         1\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00         2\n",
      "          33       1.00      0.86      0.92         7\n",
      "          34       0.00      0.00      0.00         1\n",
      "          38       1.00      1.00      1.00         5\n",
      "          39       0.50      0.50      0.50         2\n",
      "          40       0.83      0.69      0.76        36\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       1.00      0.20      0.33         5\n",
      "          44       0.64      0.74      0.69        31\n",
      "          45       0.39      0.38      0.38        24\n",
      "          47       1.00      1.00      1.00         1\n",
      "          48       0.00      0.00      0.00         6\n",
      "          49       0.68      0.72      0.70        32\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.24      0.62      0.34         8\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.55      0.50      0.52        12\n",
      "          57       0.00      0.00      0.00         1\n",
      "          61       1.00      1.00      1.00         2\n",
      "          62       1.00      1.00      1.00         2\n",
      "          65       0.33      0.18      0.24        11\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.00      0.00      0.00         1\n",
      "          68       0.83      0.50      0.62        10\n",
      "          69       0.40      1.00      0.57         2\n",
      "          71       0.00      0.00      0.00         4\n",
      "          72       0.00      0.00      0.00         1\n",
      "          75       0.00      0.00      0.00         1\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          79       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         4\n",
      "          83       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         3\n",
      "          87       0.00      0.00      0.00         2\n",
      "          88       0.64      0.67      0.65        21\n",
      "          89       0.47      0.39      0.42        18\n",
      "          92       0.00      0.00      0.00         3\n",
      "          93       0.50      1.00      0.67         2\n",
      "          95       0.88      0.58      0.70        24\n",
      "          96       0.00      0.00      0.00         0\n",
      "          97       0.50      0.43      0.46         7\n",
      "          98       0.00      0.00      0.00         4\n",
      "          99       0.44      0.62      0.52        24\n",
      "         100       0.00      0.00      0.00         1\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       1.00      0.25      0.40         4\n",
      "         103       0.00      0.00      0.00        12\n",
      "         105       0.21      0.78      0.33         9\n",
      "         106       0.67      1.00      0.80         2\n",
      "         108       0.00      0.00      0.00         2\n",
      "         111       0.00      0.00      0.00         1\n",
      "         113       0.00      0.00      0.00         1\n",
      "         116       0.00      0.00      0.00         4\n",
      "         118       0.50      0.67      0.57         3\n",
      "         119       1.00      0.94      0.97        18\n",
      "         120       0.00      0.00      0.00         2\n",
      "         123       0.00      0.00      0.00         6\n",
      "         124       0.00      0.00      0.00         4\n",
      "         125       0.00      0.00      0.00         4\n",
      "         126       0.00      0.00      0.00         1\n",
      "         127       0.50      1.00      0.67         1\n",
      "         130       0.50      0.67      0.57         3\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         3\n",
      "         133       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         2\n",
      "         136       0.00      0.00      0.00         1\n",
      "         137       0.00      0.00      0.00         1\n",
      "         142       1.00      0.40      0.57         5\n",
      "         143       0.52      0.82      0.64        17\n",
      "         144       0.61      0.67      0.64        55\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       1.00      0.31      0.48        16\n",
      "         148       0.40      0.40      0.40         5\n",
      "         149       0.86      0.35      0.50        17\n",
      "         150       0.00      0.00      0.00        10\n",
      "         151       0.00      0.00      0.00         3\n",
      "         153       0.00      0.00      0.00         1\n",
      "         156       0.45      0.31      0.37        16\n",
      "         158       0.00      0.00      0.00        11\n",
      "         159       0.00      0.00      0.00         2\n",
      "         164       0.00      0.00      0.00         1\n",
      "         166       0.40      0.67      0.50         9\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         1\n",
      "         171       1.00      0.44      0.62         9\n",
      "         172       1.00      0.25      0.40         4\n",
      "         173       0.71      0.23      0.34        22\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         1\n",
      "         176       0.00      0.00      0.00         3\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00         2\n",
      "         180       0.90      0.64      0.75        14\n",
      "         181       0.00      0.00      0.00         1\n",
      "         182       1.00      0.20      0.33         5\n",
      "         184       0.00      0.00      0.00         2\n",
      "         185       0.58      0.44      0.50        16\n",
      "         186       0.00      0.00      0.00         4\n",
      "         188       0.00      0.00      0.00        12\n",
      "         189       1.00      0.79      0.88        19\n",
      "         190       0.21      0.77      0.33        30\n",
      "         191       0.75      0.38      0.50         8\n",
      "         192       0.00      0.00      0.00         2\n",
      "         193       0.00      0.00      0.00         3\n",
      "         194       0.00      0.00      0.00         0\n",
      "         195       0.84      0.96      0.90        28\n",
      "         196       0.38      0.47      0.42        19\n",
      "         197       0.57      0.80      0.67         5\n",
      "         200       0.29      0.17      0.21        12\n",
      "         201       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         205       0.00      0.00      0.00         2\n",
      "         206       0.00      0.00      0.00         1\n",
      "         207       0.00      0.00      0.00         1\n",
      "         210       1.00      0.17      0.29         6\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       1.00      0.67      0.80         6\n",
      "         214       1.00      0.83      0.91         6\n",
      "         215       0.90      0.90      0.90        10\n",
      "         216       0.93      0.52      0.67        27\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         1\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       1.00      1.00      1.00         1\n",
      "         222       0.60      0.56      0.58        16\n",
      "         224       0.00      0.00      0.00         3\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.78      0.64      0.71        45\n",
      "         228       0.00      0.00      0.00         2\n",
      "         230       0.00      0.00      0.00         6\n",
      "         231       1.00      0.70      0.82        10\n",
      "         232       0.20      0.12      0.15         8\n",
      "         234       0.00      0.00      0.00         1\n",
      "         235       0.00      0.00      0.00         2\n",
      "         236       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.95      0.95      0.95        39\n",
      "         240       0.00      0.00      0.00         3\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.59      0.73      0.66        30\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         1\n",
      "         248       0.80      0.70      0.74        23\n",
      "         249       0.00      0.00      0.00         1\n",
      "         251       0.09      0.57      0.15        21\n",
      "         252       0.40      0.40      0.40         5\n",
      "         254       1.00      1.00      1.00         1\n",
      "         255       0.00      0.00      0.00         3\n",
      "         258       1.00      1.00      1.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       1.00      0.25      0.40         8\n",
      "         262       0.00      0.00      0.00         5\n",
      "         263       0.00      0.00      0.00         3\n",
      "         264       0.00      0.00      0.00         6\n",
      "         266       0.00      0.00      0.00         1\n",
      "         267       0.00      0.00      0.00         0\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       0.00      0.00      0.00         2\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.61      0.78      0.68        18\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         2\n",
      "         279       1.00      0.06      0.11        17\n",
      "         280       1.00      0.50      0.67         2\n",
      "         281       1.00      0.50      0.67         4\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         3\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.67      0.80      0.73         5\n",
      "         291       0.00      0.00      0.00         1\n",
      "         292       1.00      1.00      1.00         2\n",
      "         293       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         1\n",
      "         295       0.60      0.53      0.56        17\n",
      "         296       1.00      0.57      0.73         7\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         1\n",
      "         300       0.00      0.00      0.00         1\n",
      "         301       0.00      0.00      0.00         2\n",
      "         305       1.00      0.67      0.80         3\n",
      "         307       0.36      0.33      0.35        12\n",
      "         308       0.00      0.00      0.00         4\n",
      "         309       0.64      0.76      0.70        51\n",
      "         310       0.00      0.00      0.00         2\n",
      "         312       1.00      0.67      0.80         3\n",
      "         313       0.00      0.00      0.00         3\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.00      0.00      0.00         7\n",
      "         318       0.00      0.00      0.00         2\n",
      "         319       0.00      0.00      0.00         4\n",
      "         321       0.20      0.12      0.15         8\n",
      "         322       0.50      0.25      0.33         8\n",
      "         323       0.33      0.50      0.40         2\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       1.00      0.40      0.57         5\n",
      "         328       0.00      0.00      0.00         2\n",
      "         329       0.00      0.00      0.00         1\n",
      "         330       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       0.00      0.00      0.00         3\n",
      "         334       0.11      0.22      0.14         9\n",
      "         335       0.80      0.70      0.74        50\n",
      "         336       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         1\n",
      "         339       0.20      0.33      0.25         3\n",
      "         341       0.00      0.00      0.00         3\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         1\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       0.74      0.85      0.79        20\n",
      "         346       0.00      0.00      0.00         3\n",
      "         347       0.00      0.00      0.00         2\n",
      "         348       0.50      0.50      0.50         2\n",
      "         349       0.50      0.33      0.40         3\n",
      "         352       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         359       0.75      0.90      0.82        40\n",
      "         360       0.00      0.00      0.00         4\n",
      "         361       0.75      0.40      0.52        15\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.67      1.00      0.80         4\n",
      "         364       0.81      0.62      0.70        21\n",
      "         365       0.13      0.33      0.19       119\n",
      "         366       0.50      0.25      0.33         4\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.60      0.81      0.69       133\n",
      "         369       1.00      0.33      0.50         3\n",
      "         371       0.00      0.00      0.00         4\n",
      "         372       0.20      0.17      0.18         6\n",
      "         374       0.00      0.00      0.00         2\n",
      "         375       0.59      0.83      0.69        46\n",
      "         376       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.49      1999\n",
      "   macro avg       0.27      0.23      0.23      1999\n",
      "weighted avg       0.52      0.49      0.48      1999\n",
      "\n",
      "Tuned Model F1 Score for DecisionTree: 0.47581651290506055\n",
      "\n",
      "Best model: SVC with F1 Score: 0.6393482935360599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "f:\\Computer Learning Tutorials\\Projects\\talentica_software_assignment\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define classifiers and their hyperparameters for tuning\n",
    "classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'SVC': SVC(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'DecisionTree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {'clf__C': [0.1, 1, 10], 'clf__solver': ['liblinear']},\n",
    "    'MultinomialNB': {'clf__alpha': [0.5, 1.0, 1.5]},\n",
    "    'SVC': {'clf__C': [0.1, 1, 10], 'clf__kernel': ['linear', 'rbf']},\n",
    "    'RandomForest': {'clf__n_estimators': [50, 100, 200], 'clf__max_depth': [10, 20, 30]},\n",
    "    'DecisionTree': {'clf__max_depth': [10, 20, 30], 'clf__min_samples_split': [2, 5, 10]}\n",
    "}\n",
    "\n",
    "# Process data\n",
    "data, le_cat1, le_cat2, le_cat3 = preprocess_data(data)\n",
    "\n",
    "# Save LabelEncoders\n",
    "pickle.dump(le_cat1, open('le_cat1.pkl', 'wb'))\n",
    "pickle.dump(le_cat2, open('le_cat2.pkl', 'wb'))\n",
    "pickle.dump(le_cat3, open('le_cat3.pkl', 'wb'))\n",
    "\n",
    "(X_train, X_test, y_train_cat1, y_test_cat1,\n",
    " X_train_cat2, X_test_cat2, y_train_cat2, y_test_cat2,\n",
    " X_train_cat3, X_test_cat3, y_train_cat3, y_test_cat3) = split_data(data)\n",
    "\n",
    "# Train and evaluate models for Cat1\n",
    "print(\"Training and evaluating models for Cat1\")\n",
    "best_model_cat1 = build_and_evaluate_model(X_train, X_test, y_train_cat1, y_test_cat1, classifiers, param_grids)\n",
    "pickle.dump(best_model_cat1, open('best_model_cat1.pkl', 'wb'))\n",
    "\n",
    "# Convert predictions to strings and add as new features\n",
    "X_train_cat2 = X_train.to_frame()\n",
    "X_test_cat2 = X_test.to_frame()\n",
    "\n",
    "X_train_cat2['pred_cat1'] = best_model_cat1.predict(X_train).astype(str)\n",
    "X_test_cat2['pred_cat1'] = best_model_cat1.predict(X_test).astype(str)\n",
    "\n",
    "X_train_cat2_combined = X_train_cat2.apply(lambda row: ' '.join(row.values), axis=1)\n",
    "X_test_cat2_combined = X_test_cat2.apply(lambda row: ' '.join(row.values), axis=1)\n",
    "\n",
    "# Train and evaluate models for Cat2\n",
    "print(\"\\nTraining and evaluating models for Cat2\")\n",
    "best_model_cat2 = build_and_evaluate_model(X_train_cat2_combined, X_test_cat2_combined, y_train_cat2, y_test_cat2, classifiers, param_grids)\n",
    "pickle.dump(best_model_cat2, open('best_model_cat2.pkl', 'wb'))\n",
    "\n",
    "# Convert predictions to strings and add as new features\n",
    "X_train_cat3 = X_train.to_frame()\n",
    "X_test_cat3 = X_test.to_frame()\n",
    "\n",
    "X_train_cat3['pred_cat2'] = best_model_cat2.predict(X_train_cat2_combined).astype(str)\n",
    "X_test_cat3['pred_cat2'] = best_model_cat2.predict(X_test_cat2_combined).astype(str)\n",
    "\n",
    "X_train_cat3_combined = X_train_cat3.apply(lambda row: ' '.join(row.values), axis=1)\n",
    "X_test_cat3_combined = X_test_cat3.apply(lambda row: ' '.join(row.values), axis=1)\n",
    "\n",
    "# Train and evaluate models for Cat3\n",
    "print(\"\\nTraining and evaluating models for Cat3\")\n",
    "best_model_cat3 = build_and_evaluate_model(X_train_cat3_combined, X_test_cat3_combined, y_train_cat3, y_test_cat3, classifiers, param_grids)\n",
    "pickle.dump(best_model_cat3, open('best_model_cat3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perforing Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Categories: pet supplies > bunny rabbit central > food\n"
     ]
    }
   ],
   "source": [
    "# Example prediction for a new text\n",
    "def predict_new_text(title, body):\n",
    "    cleaned_title = preprocess_text(title)\n",
    "    cleaned_body = preprocess_text(body)\n",
    "    combined_cleaned_text = cleaned_title + \" \" + cleaned_body\n",
    "\n",
    "    model_cat1 = pickle.load(open('best_model_cat1.pkl', 'rb'))\n",
    "    model_cat2 = pickle.load(open('best_model_cat2.pkl', 'rb'))\n",
    "    model_cat3 = pickle.load(open('best_model_cat3.pkl', 'rb'))\n",
    "\n",
    "    le_cat1 = pickle.load(open('le_cat1.pkl', 'rb'))\n",
    "    le_cat2 = pickle.load(open('le_cat2.pkl', 'rb'))\n",
    "    le_cat3 = pickle.load(open('le_cat3.pkl', 'rb'))\n",
    "\n",
    "    # Predict Category 1\n",
    "    pred_cat1_encoded = model_cat1.predict([combined_cleaned_text])[0]\n",
    "    pred_cat1 = le_cat1.inverse_transform([pred_cat1_encoded])[0]\n",
    "    # Predict Category 2\n",
    "    combined_text_with_cat1 = combined_cleaned_text + \" \" + str(pred_cat1)\n",
    "    pred_cat2_encoded = model_cat2.predict([combined_text_with_cat1])[0]\n",
    "    pred_cat2 = le_cat2.inverse_transform([pred_cat2_encoded])[0]\n",
    "    \n",
    "    # Predict Category 3\n",
    "    combined_text_with_cat2 = combined_cleaned_text + \" \" + str(pred_cat2)\n",
    "    pred_cat3_encoded = model_cat3.predict([combined_text_with_cat2])[0]\n",
    "    pred_cat3 = le_cat3.inverse_transform([pred_cat3_encoded])[0]\n",
    "\n",
    "    return f\"Predicted Categories: {pred_cat1} > {pred_cat2} > {pred_cat3}\"\n",
    "\n",
    "# Example usage\n",
    "new_text_title = \"Kaytee Timothy Cubes, 1-Pound\"\n",
    "new_text_body = \"My bunny had a hard time eating this because the hay was so dry and it was too small for her to chew on.\"\n",
    "predicted_categories = predict_new_text(new_text_title, new_text_body)\n",
    "print(predicted_categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
